{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Basic Neural Networks with LENGTH\n",
    "\n",
    "This is the first practical exercise of our course [Applied Edge AI](https://learn.ki-campus.org/courses/edgeai-hpi2022).\n",
    "In this exercise, you are going to implement a few basic functions and library components of neural networks from scratch in Python.\n",
    "\n",
    "Before we can actually start, install our pip package, which installs the surrounding library LENGTH (Lightning-fast Extensible Neural-network Guarding The HPI), by running the following cell (click the triangular \"Play\" button next to the cell or in the top bar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "4b88e7db-9419-493a-b4fb-aaf1a8efd76c",
    "_uuid": "c71811f0-2527-41cc-a68a-3d70412c15cd",
    "execution": {
     "iopub.execute_input": "2022-01-19T09:34:35.447635Z",
     "iopub.status.busy": "2022-01-19T09:34:35.447152Z",
     "iopub.status.idle": "2022-01-19T09:34:52.111541Z",
     "shell.execute_reply": "2022-01-19T09:34:52.110069Z",
     "shell.execute_reply.started": "2022-01-19T09:34:35.447555Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install length_hpi\n",
    "# if you get the warning \"Failed to establish a new connection\", go to the side bar on the right, then \"Settings\" and switch on \"Internet\"\n",
    "# you can safely ignore errors such as \"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. [...]\"\n",
    "# since we only need our length package, Pillow, and numpy in this exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to prepare a few imports for the following code cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "15444bfe-13a7-4ae5-9d90-3c75b94b18c3",
    "_uuid": "4496c81d-1702-472b-a547-f46cbf085b36",
    "execution": {
     "iopub.execute_input": "2022-01-19T09:34:52.114451Z",
     "iopub.status.busy": "2022-01-19T09:34:52.114112Z",
     "iopub.status.idle": "2022-01-19T09:34:52.135685Z",
     "shell.execute_reply": "2022-01-19T09:34:52.134313Z",
     "shell.execute_reply.started": "2022-01-19T09:34:52.114419Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import length.functions as F\n",
    "\n",
    "import length.tests.optimizer_tests.test_sgd as sgd_tests\n",
    "import length.tests.layer_tests.test_fully_connected as fc_tests\n",
    "import length.tests.function_tests.test_mean_squared_error as mse_tests\n",
    "import length.tests.function_tests.test_relu as relu_tests\n",
    "\n",
    "from length import constants\n",
    "from length.data_sets import MNIST, FashionMNIST\n",
    "from length.function import Function\n",
    "from length.models import MLP\n",
    "from length.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: SGD\n",
    "\n",
    "In one of the lectures in our course we learned about SGD.\n",
    "In the following task we want to compute the parameter delta to actually implement SGD.\n",
    "If you look up the formula on our course slides, the `param_deltas` are subtracted by our framework, thus we do *not* need to multiply our result with -1 in the code.\n",
    "Also note, that the variable `gradients` is the *list* of computed derivatives, thus the derivative part of the formula is already computed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2022-01-19T09:34:52.137788Z",
     "iopub.status.busy": "2022-01-19T09:34:52.137344Z",
     "iopub.status.idle": "2022-01-19T09:34:52.147535Z",
     "shell.execute_reply": "2022-01-19T09:34:52.146391Z",
     "shell.execute_reply.started": "2022-01-19T09:34:52.137748Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2edde99b8dda21ba07aee90541ff552",
     "grade": true,
     "grade_id": "cell-cdd4ef62122aa747",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    \"\"\"\n",
    "    An optimizer that does plain Stochastic Gradient Descent\n",
    "    (https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Iterative_method)\n",
    "    :param lr: the learning rate to use for optimization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr):\n",
    "        self.learning_rate = lr\n",
    "\n",
    "    def run_update_rule(self, gradients, _):\n",
    "        # :param gradients: a list of all computed gradient arrays\n",
    "        # :return: a list of deltas for each provided gradient\n",
    "\n",
    "        # TODO: implement SGD update rule and store the result in a variable called param_deltas (as a list)\n",
    "        # HINT: it can be solved in a single line with a list comprehension ;)\n",
    "        # TASK START - Start coding here:\n",
    "        param_deltas = [self.learning_rate*grad for grad in gradients]\n",
    "        return param_deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test your solution, you can execute the following code cell.\n",
    "If your solution passes our test, it will simply print `Test passed.`\n",
    "If it does not work, you will get an error.\n",
    "In this case you need to fix the code above.\n",
    "(And do not forget to run the above code cell again!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T09:34:52.150530Z",
     "iopub.status.busy": "2022-01-19T09:34:52.150242Z",
     "iopub.status.idle": "2022-01-19T09:34:52.170783Z",
     "shell.execute_reply": "2022-01-19T09:34:52.170091Z",
     "shell.execute_reply.started": "2022-01-19T09:34:52.150495Z"
    }
   },
   "outputs": [],
   "source": [
    "sgd_tests.SGD = SGD\n",
    "sgd_tests.test_sgd()\n",
    "print(\"Test passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Fully Connected Layer\n",
    "\n",
    "It seems we have not learned a lot about a \"fully connected\" layer (sometimes also called a \"Dense\" layer) in our lecture so far.\n",
    "However we learned about Perceptrons and Multi Layer Perceptrons (MLPs), and fully connected layers are the building block of these early models for machine learning.\n",
    "\n",
    "They simply store a weight between each possible input and output value and a bias for each output value.\n",
    "For example, if we have 2 inputs $i_0,i_1$ and 3 outputs $o_0,o_1,o_2$, we store 6 weight values $w_{00}, w_{01}, w_{10}, w_{11}, w_{20},w_{21}$, one value for each pair of one input and one output, and three bias values $b_0,b_1,b_2$, one for each output.\n",
    "Then during the forward pass the outputs of a fully connected layer are calculated as\n",
    "$$o_x = \\sum_{y=0}^1{(i_y \\cdot w_{xy}) + b_x} \\text{ for } 0 \\leq x < 3.$$\n",
    "This is simplified for a single element but in a neural network we work with mini-batches (processing a small number of samples at the same time).\n",
    "In this case, we can use the [dot product](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) of two arrays to simplify this element-wise multiplication.\n",
    "\n",
    "Here is a small very simple code snippet for the forward pass of our example case above with a batch size of 5.\n",
    "The batch axis complicates matters slightly, but pay attention to the shape of each array and how the output size changes.\n",
    "You can play around with different input and output sizes here if you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T09:34:52.172809Z",
     "iopub.status.busy": "2022-01-19T09:34:52.172242Z",
     "iopub.status.idle": "2022-01-19T09:34:52.188904Z",
     "shell.execute_reply": "2022-01-19T09:34:52.188208Z",
     "shell.execute_reply.started": "2022-01-19T09:34:52.172775Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "num_inputs = 2\n",
    "num_outputs = 3\n",
    "\n",
    "i = np.arange(batch_size * num_inputs).reshape(batch_size, num_inputs)\n",
    "w = np.arange(num_outputs * num_inputs).reshape(num_outputs, num_inputs)\n",
    "b = np.zeros(num_outputs)\n",
    "print(f\"Inputs {i.shape}:\\n\", i)\n",
    "print(f\"Weights {w.shape}:\\n\", w)\n",
    "print(f\"Bias {b.shape}:\\n\", b)\n",
    "\n",
    "output = np.dot(i, w.T) + b\n",
    "print(f\"Output {output.shape}:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One interesting fact here can help you with implementing the backward pass:\n",
    "applying the dot product on two arrays with shapes (5,2) and (2,3) \"removes\" the common axis with size 2 and results in an array of (5,3).\n",
    "You can use this knowledge to figure out which arrays need to be transposed to get the correct shape during the *backward* pass.\n",
    "\n",
    "Furthermore, you can recap how multiplication (between weights and inputs) affects the gradients in our lecture video [Computational Graph](https://learn.ki-campus.org/courses/edgeai-hpi2022/items/3btmrU8Ds8rVDk8SEUz1pU).\n",
    "(Remember instead of using simple multiplication we can use the dot product for arrays.)\n",
    "In the same video you can also recap what happens when we add *two* values in a computational graph (in this case the result of the dot product and our bias), so you can later on implement the *backward* pass for the *bias* correctly.\n",
    "\n",
    "Also you may have noted, that we *transposed* the weight array in the example above - \"swapping\" the axes from (3,2) to (2,3) - with numpy before the dot product by calling `.T`.\n",
    "\n",
    "This is because we recommend storing the weight array in a *transposed* way in your implementation (similar to our example above).\n",
    "\n",
    "One final hint before you are ready to start implementing: we use the variable name `x` for the inputs (`grad_x` for the gradients with respect to the inputs) in our code below, instead of `i` in the example above (we only used `i` above so it maches the previous formula)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the neuron (without activation function):\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_{(o, bs)} = \\mathbf{W}_{(i,o)}^T \\cdot \\mathbf{x}_{(i,bs)} + \\mathbf{b}_{(o,bs)} \n",
    "$$\n",
    "\n",
    "Where $i$, $o$, and $bs$ are the number of inputs, outputs and batch elements, respectively.\n",
    "The derivatives of $\\mathbf{y}$ related to the variables ot its function are,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\mathbf{W}_{(i,o)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{W}} = \\mathbf{x}_{(i, bs)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{b}} = \\left\\{ 1 \\right\\}_{(o, bs)}\n",
    "$$\n",
    "\n",
    "For the backward pass, we use the chain rule. Hence, being $\\mathbf{output}_{(o, bs)}$ the vector of output of the model, we have the gradient applied to the layer in analysis\n",
    "\n",
    "$$\n",
    "\\mathbf{grad}_x = \\frac{\\partial \\mathbf{output}}{\\partial \\mathbf{x}}_{(i, bs)} = \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}_{(i, o)} \\mathbf{grad}_{(o, bs)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{grad}_W = \\frac{\\partial \\mathbf{output}}{\\partial \\mathbf{W}}_{(i, o)} = \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{W}}_{(i, bs)} \\mathbf{grad}_{(o, bs)}^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{grad}_b = \\frac{\\partial \\mathbf{output}}{\\partial \\mathbf{b}}_{(o, bs)} = \\mathbf{grad}_{(o, bs)}\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{grad}$ is the gradient obtained by applying the chain rule on the higher layers.\n",
    "**Observation** Since the bias is defined for just one sample, the $\\mathbf{grad}_b$ is reduced to the mean related to the 0 axis (that is, over the batch sample).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2022-01-19T09:34:52.191944Z",
     "iopub.status.busy": "2022-01-19T09:34:52.190583Z",
     "iopub.status.idle": "2022-01-19T09:34:52.208673Z",
     "shell.execute_reply": "2022-01-19T09:34:52.208129Z",
     "shell.execute_reply.started": "2022-01-19T09:34:52.191869Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4d4e532f3a751325080681aa0188f93",
     "grade": true,
     "grade_id": "cell-5391cae9581cdcc0",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from length.layer import Layer\n",
    "from length.constants import DTYPE\n",
    "from length.initializers.xavier import Xavier\n",
    "\n",
    "\n",
    "class FullyConnected(Layer):\n",
    "    \"\"\"\n",
    "    The FullyConnected Layer is one of the base building blocks of a neural network. It computes a weighted sum\n",
    "    over the input, using a weight matrix. It furthermore applies a bias term to this weighted sum to allow linear\n",
    "    shifts of the computed values.\n",
    "    \"\"\"\n",
    "    name = \"FullyConnected\"\n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, weight_init=Xavier()):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: initialize our weights with correct shape, using the weight initializer 'weight_init'\n",
    "        # here are two hints:\n",
    "        # 1. store an array of zeros in `self._weights` with the correct shape (we recommend storing it transposed as in our simple example above) and use dtype=DTYPE\n",
    "        # 2. call `weight_init` with our freshly created array `self._weights` to initialize the array properly\n",
    "        self._weights = np.zeros(shape=(num_outputs, num_inputs,), dtype=DTYPE)\n",
    "        weight_init(self._weights)\n",
    "        \n",
    "        # TODO: initialize `self.bias` with an array of zeros in the correct shape and use dtype=DTYPE\n",
    "        self.bias = np.zeros(shape=(num_outputs,), dtype=DTYPE)\n",
    "        \n",
    "    @property\n",
    "    def weights(self):\n",
    "        # Transform weights between internal and external representation\n",
    "        return self._weights.T\n",
    "\n",
    "    @weights.setter\n",
    "    def weights(self, value):\n",
    "        # Transform weights between internal and external representation\n",
    "        self._weights = value.T\n",
    "\n",
    "    def internal_forward(self, inputs):\n",
    "        x, = inputs\n",
    "        \n",
    "        # TODO: calculate the output of this layer and store it in a variable `result`\n",
    "        #       (hint: you can look at our simple example above)\n",
    "        result = np.dot(x, self._weights.T) + self.bias\n",
    "        \n",
    "        return result,\n",
    "\n",
    "    def internal_backward(self, inputs, gradients):\n",
    "        x, = inputs\n",
    "        grad_in, = gradients\n",
    "        \n",
    "        # TODO: calculate gradients with respect to inputs for this layer\n",
    "        # 1. calculate and store gradients for (the batch of) the inputs `x` in `grad_x`\n",
    "        #    (hint: instead of simple multiplication we need to use the dot product for arrays)\n",
    "        # 2. calculate and store gradients for the weights `w` in `grad_w`\n",
    "        #    (hint: the shapes of `grad_w` and `self._weights` must be equal, so try to figure out which axes is \"removed\" by applying the dot product)\n",
    "        # 3. calculate and store gradients for the bias `b` in `grad_b`\n",
    "        #    (hint: gradients from multiple sources in the computational graph need to be added up)\n",
    "        grad_x = np.dot(grad_in, self._weights)\n",
    "        grad_w = np.dot(grad_in.T, x)\n",
    "        grad_b = np.sum(grad_in, axis=0)\n",
    "        \n",
    "        assert grad_x.shape == x.shape\n",
    "        assert grad_w.shape == self._weights.shape\n",
    "        assert grad_b.shape == self.bias.shape\n",
    "\n",
    "        return grad_x, grad_w, grad_b\n",
    "\n",
    "    def internal_update(self, parameter_deltas):\n",
    "        delta_w, delta_b = parameter_deltas\n",
    "        \n",
    "        # TODO: apply updates to weights (self._weights) and bias (self.bias) according to deltas from optimizer\n",
    "        # if you remember our instructions on how to implement SGD, we said: \"[...] the param_deltas are subtracted by our framework [...]\"\n",
    "        # so this is all we need to do here.\n",
    "        self._weights = self._weights - delta_w \n",
    "        self.bias = self.bias - delta_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test your solution, you can execute the following code cell.\n",
    "If your solution passes our test, it will simply print `Test passed.`\n",
    "If it does not work, you will get an error.\n",
    "In this case you need to fix the code above.\n",
    "(And do not forget to run the above code cell again!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T09:34:52.209920Z",
     "iopub.status.busy": "2022-01-19T09:34:52.209697Z",
     "iopub.status.idle": "2022-01-19T09:34:52.358902Z",
     "shell.execute_reply": "2022-01-19T09:34:52.358225Z",
     "shell.execute_reply.started": "2022-01-19T09:34:52.209897Z"
    }
   },
   "outputs": [],
   "source": [
    "fc_tests.FullyConnected = FullyConnected\n",
    "fc_tests.test_initialization()\n",
    "fc_tests.test_fully_connected_forward()\n",
    "fc_tests.test_fully_connected_backward()\n",
    "print(\"Test passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Mean Squared Error\n",
    "\n",
    "To train a model, we also need a loss function.\n",
    "These loss functions mathematically define how our model should be optimized (and thus learn to solve a certain task).\n",
    "\n",
    "A very simple loss function, which still can be effective in training models is the [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error).\n",
    "Therefore in the following task, we are going to implement this function.\n",
    "The corresponding [wikipedia article](https://en.wikipedia.org/wiki/Mean_squared_error) should explain everything you need to know, if you are not yet familiar with the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2022-01-19T09:34:52.360521Z",
     "iopub.status.busy": "2022-01-19T09:34:52.360272Z",
     "iopub.status.idle": "2022-01-19T09:34:52.372830Z",
     "shell.execute_reply": "2022-01-19T09:34:52.371785Z",
     "shell.execute_reply.started": "2022-01-19T09:34:52.360488Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "709d25034c53f755e3671ff45be412c8",
     "grade": true,
     "grade_id": "cell-6fe26c809f0b5386",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MeanSquaredError(Function):\n",
    "    \"\"\"\n",
    "    This function calculates the Mean Squared Error between two given vectors, as described in\n",
    "    https://en.wikipedia.org/wiki/Mean_squared_error\n",
    "    \"\"\"\n",
    "    name = \"MeanSquaredError\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: add more initialization if necessary\n",
    "        self.error = None\n",
    "\n",
    "    @staticmethod\n",
    "    def create_one_hot(data, shape):\n",
    "        assert len(shape) == 2, \"Providing integers as second input to MSE only works with two dimensional input vectors\"\n",
    "        # TODO: create a one-hot representation out of the given label vector (with dtype=DTYPE)\n",
    "        # Example: assume `data` is [2, 3, 0], and the desired `shape` is (3, 4)\n",
    "        #          in this case we have 4 possible classes and 3 samples, belonging to class 2, class 3, and class 0 \n",
    "        #          therefore we need to set a 1 at position 2, 3, 0 for each sample respectively\n",
    "        #          the resulting vector should look like this:\n",
    "        #          result = [[0, 0, 1, 0], [0, 0, 0, 1], [1, 0, 0, 0]]\n",
    "        # Hint: initialize an array of zeros with the given `shape`, set 1s where needed and in the end return your created array\n",
    "        result = np.zeros(shape, dtype=DTYPE)\n",
    "        for i in range(len(data)):\n",
    "            result[i, data[i]] = 1\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    def internal_forward(self, inputs):\n",
    "        x1, x2 = inputs\n",
    "\n",
    "        if np.issubdtype(x2.dtype, np.integer):\n",
    "            x2 = self.create_one_hot(x2, x1.shape)\n",
    "\n",
    "        self.error = np.mean((x2 - x1)**2).astype(DTYPE)\n",
    "        \n",
    "        return self.error,\n",
    "\n",
    "    def internal_backward(self, inputs, gradients):\n",
    "        x1, x2 = inputs\n",
    "        gx, = gradients\n",
    "        \n",
    "        # TODO: calculate the gradients of this function with respect to its (two) inputs\n",
    "        # (hint: the derivative depends on the inputs (here you could use an intermediate \n",
    "        #        result from the forward pass) and the size of the inputs)\n",
    "        \n",
    "        \n",
    "        if np.issubdtype(x2.dtype, np.integer):\n",
    "            # in case we used MSE as loss function, we won't propagate any gradients to the loss\n",
    "            x2 = self.create_one_hot(x2, x1.shape)\n",
    "            gradient_1 = 2/x1.size * -1 *(x2 - x1) * gx\n",
    "            \n",
    "            return gradient_1, None\n",
    "        \n",
    "        gradient_1 = 2/x1.size * -1 *(x2 - x1) * gx\n",
    "        gradient_2 = 2/x2.size * (x2 - x1) * gx\n",
    "\n",
    "        return gradient_1, gradient_2\n",
    "\n",
    "\n",
    "def mean_squared_error(input_1, input_2):\n",
    "    \"\"\"\n",
    "    This function calculates the Mean Squared Error between input_1 and input_2. Both inputs should be vectors of the\n",
    "    same shape. You can also supply a one-dimensional list of integers.\n",
    "    If you do so this vector will be converted to a one_hot representation that fits to the shape of the second\n",
    "    input\n",
    "    :param input_1: the first vector of any shape\n",
    "    :param input_2: the second vector. Needs to have the same shape as the first vector, or be a one-dimensional int vector\n",
    "    :return: the mean squared error between input_1 and input_2\n",
    "    \"\"\"\n",
    "    return MeanSquaredError()(input_1, input_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test your solution, you can execute the following code cell.\n",
    "If your solution passes our test, it will simply print `Test passed.`\n",
    "If it does not work, you will get an error.\n",
    "In this case you need to fix the code above.\n",
    "(And do not forget to run the above code cell again!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T09:34:52.374742Z",
     "iopub.status.busy": "2022-01-19T09:34:52.374444Z",
     "iopub.status.idle": "2022-01-19T09:34:52.399541Z",
     "shell.execute_reply": "2022-01-19T09:34:52.398942Z",
     "shell.execute_reply.started": "2022-01-19T09:34:52.374709Z"
    }
   },
   "outputs": [],
   "source": [
    "mse_tests.MeanSquaredError = MeanSquaredError\n",
    "mse_tests.mean_squared_error = mean_squared_error\n",
    "mse_tests.test_mean_squared_error_forward_zero_loss()\n",
    "mse_tests.test_mean_squared_error_forward_loss()\n",
    "mse_tests.test_mean_squared_error_forward_int_input()\n",
    "mse_tests.test_mean_squared_error_backward()\n",
    "mse_tests.test_mean_squared_error_backward_with_label()\n",
    "print(\"Test passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: ReLU (Rectified Linear Unit)\n",
    "\n",
    "In our course we learned about how the ReLU function helped to solve the problem of vanishing gradients in the video [A Concise History of Neural Networks (3/4)](https://learn.ki-campus.org/courses/edgeai-hpi2022/items/22nlBMim7pwAX8A7gTI7qn).\n",
    "\n",
    "In the next task, we are going to implement this function by filling in the missing forward and backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2022-01-19T09:34:52.402191Z",
     "iopub.status.busy": "2022-01-19T09:34:52.401942Z",
     "iopub.status.idle": "2022-01-19T09:34:52.411559Z",
     "shell.execute_reply": "2022-01-19T09:34:52.410476Z",
     "shell.execute_reply.started": "2022-01-19T09:34:52.402167Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5814e5a8dd35b4fe3c52538ac83c832",
     "grade": true,
     "grade_id": "cell-706ebecfe1eb1671",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Relu(Function):\n",
    "    \"\"\"\n",
    "    The Relu Layer is a non-linear activation\n",
    "    \"\"\"\n",
    "    name = \"ReLU\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: add more initialization of class member variables if necessary\n",
    "        self.output = None\n",
    "        \n",
    "    def internal_forward(self, inputs):\n",
    "        x, = inputs\n",
    "        # TODO: calculate forward pass of ReLU function and store it in `activated_inputs`\n",
    "        # (we can store any variables we need for the calculation of the backward pass in a class member variable here as well)\n",
    "        x_gt_zero = x > 0 \n",
    "        relu_x = x.copy()\n",
    "        relu_x[~x_gt_zero] = 0\n",
    "        self.output = relu_x \n",
    "        return self.output, \n",
    "        \n",
    "    def internal_backward(self, inputs, gradients):\n",
    "        x, = inputs\n",
    "        grad_in, = gradients\n",
    "        \n",
    "        # TODO: calculate gradients of ReLU function with respect to the input and store it in grad_x\n",
    "        # you can first calculate the derivative of ReLU itself (hint: it depends on the input of the forward pass)\n",
    "        # and then use element-wise multiplication of the calculated derivative with the `grad_in` gradients\n",
    "        relu_back = self.output.copy()\n",
    "        o_gt_zero = relu_back > 0\n",
    "        relu_back[o_gt_zero] = 1\n",
    "        grad_x = np.multiply(grad_in, relu_back)\n",
    "        assert grad_x.shape == x.shape\n",
    "        return grad_x,\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    This function computes the element-wise ReLU activation function (https://en.wikipedia.org/wiki/Rectifier_(neural_networks))\n",
    "    on a given input vector x.\n",
    "    :param x: the input vector\n",
    "    :return: a rectified version of the input vector\n",
    "    \"\"\"\n",
    "    return Relu()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test your solution, you can execute the following code cell.\n",
    "If your solution passes our test, it will simply print `Test passed.`\n",
    "If it does not work, you will get an error.\n",
    "In this case you need to fix the code above.\n",
    "(And do not forget to run the above code cell again!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T09:34:52.413441Z",
     "iopub.status.busy": "2022-01-19T09:34:52.413171Z",
     "iopub.status.idle": "2022-01-19T09:34:52.434933Z",
     "shell.execute_reply": "2022-01-19T09:34:52.434045Z",
     "shell.execute_reply.started": "2022-01-19T09:34:52.413409Z"
    }
   },
   "outputs": [],
   "source": [
    "relu_tests.relu = relu\n",
    "relu_tests.Relu = Relu\n",
    "relu_tests.test_relu_forward()\n",
    "relu_tests.test_relu_backward()\n",
    "print(\"Test passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a MultiLayerPerceptron (MLP)\n",
    "\n",
    "Now we have implemented all the building blocks required to build and train a MultiLayerPerceptron.\n",
    "Please explore the code below a bit, especially how our layers are initialized in the `__init__` function, and how they are used during the forward pass.\n",
    "Also you can see how our functions (`relu` and `mean_squared_error`) are used, but do not need initialization or saving, since they - in contrast to our `FullyConnected` layers - store no weights .\n",
    "\n",
    "We only need to save the output of our loss function in `self.loss` to be able to calculate gradients in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T09:34:52.436170Z",
     "iopub.status.busy": "2022-01-19T09:34:52.435925Z",
     "iopub.status.idle": "2022-01-19T09:34:52.449521Z",
     "shell.execute_reply": "2022-01-19T09:34:52.448414Z",
     "shell.execute_reply.started": "2022-01-19T09:34:52.436140Z"
    }
   },
   "outputs": [],
   "source": [
    "class ActivatedMLP:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fully_connected_1 = FullyConnected(784, 512)\n",
    "        self.fully_connected_2 = FullyConnected(512, 512)\n",
    "        self.fully_connected_3 = FullyConnected(512, 10)\n",
    "\n",
    "        self.loss = None\n",
    "        self.predictions = None\n",
    "    \n",
    "    def forward(self, batch, train=True):\n",
    "        hidden = relu(self.fully_connected_1(batch.data))\n",
    "        hidden = relu(self.fully_connected_2(hidden))\n",
    "        self.predictions = self.fully_connected_3(hidden)\n",
    "        self.loss = mean_squared_error(self.predictions, batch.labels)\n",
    "        \n",
    "    def backward(self, optimizer):\n",
    "        self.loss.backward(optimizer)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loop\n",
    "\n",
    "Now we still need a training loop implementation.\n",
    "We have provided one below, but you should carefully try to understand the concepts of what is happening here.\n",
    "\n",
    "(Next week, we are going to implement our own training loop with PyTorch in the exercise. It is going to look slightly different.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T09:34:52.452630Z",
     "iopub.status.busy": "2022-01-19T09:34:52.451995Z",
     "iopub.status.idle": "2022-01-19T09:34:52.472213Z",
     "shell.execute_reply": "2022-01-19T09:34:52.471095Z",
     "shell.execute_reply.started": "2022-01-19T09:34:52.452590Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(args, data_set, model, optimizer):\n",
    "    for epoch in range(args.num_epochs):\n",
    "        # train our model\n",
    "        for iteration, batch in enumerate(data_set.train):\n",
    "            model.forward(batch)\n",
    "            model.backward(optimizer)\n",
    "\n",
    "            if iteration % args.train_verbosity == 0:\n",
    "                accuracy = F.accuracy(model.predictions, batch.labels).data\n",
    "                print(\"train: epoch: {: 2d}, loss: {: 5.2f}, accuracy {:.2f}, iteration: {: 4d}\".\n",
    "                      format(epoch, model.loss.data, accuracy, iteration), end=\"\\r\")\n",
    "        \n",
    "        # test our model\n",
    "        print(\"\\nrunning test set...\")\n",
    "        sum_accuracy = 0.0\n",
    "        sum_loss = 0.0\n",
    "        for iterations, batch in enumerate(data_set.test):\n",
    "            model.forward(batch, train=False)\n",
    "            sum_accuracy += F.accuracy(model.predictions, batch.labels).data\n",
    "            sum_loss += model.loss.data\n",
    "        nr_batches = iterations - 1\n",
    "        print(\" test: epoch: {: 2d}, loss: {: 5.2f}, accuracy {:.2f}\".\n",
    "              format(epoch, sum_loss / nr_batches, sum_accuracy / nr_batches))\n",
    "        \n",
    "        # a very simple update rule for the learning rate, reduce it by a factor of 10 after the first and the fourth epoch\n",
    "        # do *not* change this if you want to solve our bonus task (otherwise it will change the expected results)\n",
    "        if epoch % 3 == 0:\n",
    "            optimizer.learning_rate *= 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Training\n",
    "\n",
    "Finally, we can train our MLP.\n",
    "We have provided code to load two datasets [MNIST](http://yann.lecun.com/exdb/mnist/) and [FasionMNIST](https://github.com/zalandoresearch/fashion-mnist).\n",
    "The first one classifies handwritten digits in grayscale images, the latter one classifies items of clothing based on grayscale images.\n",
    "\n",
    "Since these datasets are extremely small, we can even train our model on CPU and it should not take longer than a few minutes.\n",
    "You should be able to simply run the code below right now.\n",
    "Note how the train and test accuracy changes during the training process.\n",
    "\n",
    "Training with the default settings (using our implemented SGD optimizer starting with a learning rate of 0.1) our MLP on MNIST should achieve about 0.78 (78%) **test** accuracy after 5 epochs of training.\n",
    "\n",
    "If you implement the Adam optimizer in the bonus task at the end of this notebook, you need to edit the code below to enable training with it, look for the instructions in the comments after each line starting with `# BONUS TASK ADAM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T09:34:52.474098Z",
     "iopub.status.busy": "2022-01-19T09:34:52.473566Z",
     "iopub.status.idle": "2022-01-19T09:35:25.669765Z",
     "shell.execute_reply": "2022-01-19T09:35:25.669248Z",
     "shell.execute_reply.started": "2022-01-19T09:34:52.474056Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    args.check()\n",
    "\n",
    "    data_set = None\n",
    "    if args.data_set == \"mnist\":\n",
    "        data_set = MNIST(args.batch_size)\n",
    "    if args.data_set == \"fashion\":\n",
    "        data_set = FashionMNIST(args.batch_size)\n",
    "\n",
    "    optimizer = None\n",
    "    if args.optimizer == \"adam\":\n",
    "        # BONUS TASK ADAM: if you implemented Adam below (and ran that code cell), then remove or comment the next line...\n",
    "        # raise NotImplementedError(\"Adam not implemented yet.\")\n",
    "        # ... and uncomment the following line:\n",
    "        optimizer = Adam(args.learning_rate)\n",
    "    if args.optimizer == \"sgd\":\n",
    "        optimizer = SGD(args.learning_rate)\n",
    "\n",
    "    model = ActivatedMLP()\n",
    "\n",
    "    train_loop(args, data_set, model, optimizer)\n",
    "\n",
    "class TrainOptions:\n",
    "    # do not change these values, rather set other values below before calling the main function\n",
    "    num_epochs      = 5       # number of epochs\n",
    "    batch_size      = 64      # batch size\n",
    "    optimizer       = \"sgd\"   # adam or sgd\n",
    "    data_set        = \"mnist\" # which dataset we want to train for\n",
    "    learning_rate   = 0.1     # learning rate\n",
    "    train_verbosity = 50      # how often to print the training accuracy\n",
    "\n",
    "    def check(self):\n",
    "        assert self.optimizer in [\"adam\", \"sgd\"]\n",
    "        assert self.data_set in [\"mnist\", \"fashion\"]\n",
    "\n",
    "options = TrainOptions()\n",
    "\n",
    "# BONUS TASK ADAM: uncomment the following lines if you have implemented adam and you should be able to reach 99% test accuracy on MNIST after one or two epochs\n",
    "#options.optimizer=\"adam\"\n",
    "#options.learning_rate = 0.001\n",
    "# BONUS TASK ADAM: after you trained a model on MNIST, you can change the dataset to FashionMNIST and train another MLP to answer our graded bonus quiz\n",
    "#options.data_set = \"fashion\"\n",
    "\n",
    "main(options)\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What now?\n",
    "\n",
    "Now you should have implemented a few basic functions of a neural network from scratch.\n",
    "We hope this has provided you with a deeper knowledge of how neural network layers are implemented and how the models can be constructed and trained.\n",
    "\n",
    "You can now experiment a bit more with the options given above, for example:\n",
    "\n",
    "* try to choose a different learning rate and see how it affects the training and testing accuracy\n",
    "* train a model for the more challenging FashionMNIST dataset\n",
    "* train for a few more epochs and see if you can achieve a higher accuracy\n",
    "* try a different learning rate update rule in the training loop implementation\n",
    "\n",
    "Make sure you attempt to complete all four tasks to the best of your abilities, as we will have questions in the graded quiz which relate to this exercise.\n",
    "\n",
    "You can also attempt our bonus task below.\n",
    "\n",
    "# Optional Bonus Task: Adam\n",
    "\n",
    "In this task (which is completely optional and a bit more challenging - but at the same time highly rewarding if you manage to complete it) you can implement the famous Adam optimizer yourself.\n",
    "This optimizer was first proposed in an [arxiv preprint](https://arxiv.org/abs/1412.6980) in 2014.\n",
    "\n",
    "It is popular because it is less sensitive to choosing a suitable learning rate, and many improvements to the original implementation have since been found.\n",
    "You can absolutely implement the optimizer based on the original paper linked above, but of course you are welcome to find additional sources and explanations online if you encounter difficulties.\n",
    "\n",
    "## Mathematics and Programming\n",
    "\n",
    "To highlight the mathematical side of deep learning we present a possible mathematical reformulation when updating the biased first moment estimates in the following.\n",
    "The original paper suggests the formula:\n",
    "\n",
    "$$m_t \\leftarrow \\beta_1\\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t$$\n",
    "\n",
    "However we can simplify the equation on the right side by adding a \"zero\" ($m_{t-1} - m_{t-1}=0$)\n",
    "$$m_t \\leftarrow m_{t-1} - m_{t-1} + \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t$$\n",
    "This allows us to factor out $-m_{t-1}$\n",
    "$$m_t \\leftarrow m_{t-1} - m_{t-1} \\cdot (1-\\beta_1) + (1-\\beta_1) \\cdot g_t$$\n",
    "and also factor out $(1-\\beta_1)$:\n",
    "$$m_t \\leftarrow m_{t-1} + (1-\\beta_1) \\cdot (g_t - m_{t-1})$$\n",
    "\n",
    "This may not look simpler until we realize this allows us to use the `+=` operator in Python pseudocode:\n",
    "```\n",
    "m += (1 - self.beta1) * (g - m)\n",
    "```\n",
    "Similarly the second raw moment estimate update can be reformulated to\n",
    "\n",
    "$$v_t \\leftarrow v_{t-1} + (1-\\beta_2) \\cdot (g_t^2 - v_{t-1})$$\n",
    "\n",
    "which in Pseudocode can be written as `v += (1 - self.beta2) * (g * g - v)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2022-01-19T09:35:25.671427Z",
     "iopub.status.busy": "2022-01-19T09:35:25.670842Z",
     "iopub.status.idle": "2022-01-19T09:35:25.689527Z",
     "shell.execute_reply": "2022-01-19T09:35:25.687132Z",
     "shell.execute_reply.started": "2022-01-19T09:35:25.671396Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5589ac0e6602909adb0ab19280ee0f2",
     "grade": true,
     "grade_id": "cell-8c10d99dfa560280",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    \"\"\"\n",
    "    The Adam optimizer (see https://arxiv.org/abs/1412.6980)\n",
    "    :param learning_rate: initial step size\n",
    "    :param beta1: Exponential decay rate of the first order moment\n",
    "    :param beta2: Exponential decay rate of the second order moment\n",
    "    :param eps: Small value for numerical stability\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "\n",
    "        # map from layer id to a list of numpy arrays\n",
    "        self.m_values = {}\n",
    "        self.v_values = {}\n",
    "        # map from layer id to int (time step)\n",
    "        self.t_values = {}\n",
    "\n",
    "        self.current_id = -1\n",
    "        self._initialized = set()\n",
    "\n",
    "    def run_update_rule(self, gradients, layer):\n",
    "        \n",
    "        self.current_id = id(layer)\n",
    "\n",
    "        if not self.initialized:\n",
    "            self.initialize(gradients)\n",
    "            \n",
    "        # TODO: Implement Adam Update Rule, save result in variable param_deltas and return\n",
    "        # (for this bonus task you need to figure out the implementation details of Adam for yourself)\n",
    "        # hint: self.m_values contains a *list* of \"m_{t-1}\" arrays (exactly one value with the correct shape for each gradient in the `gradients` list)\n",
    "        alpha = self.learning_rate\n",
    "        beta1 = self.beta1\n",
    "        beta2 = self.beta2\n",
    "        eps = self.eps\n",
    "\n",
    "        t = self.t_values[self.current_id] + 1\n",
    "        m_values = self.m_values[self.current_id]\n",
    "        v_values = self.v_values[self.current_id]\n",
    "\n",
    "        m_values_t_1 = []\n",
    "        v_values_t_1 = []\n",
    "        param_deltas = []\n",
    "\n",
    "        for m, v, g in zip(m_values, v_values, gradients):\n",
    "            m += (1 - beta1)*(g - m)\n",
    "            v += (1 - beta2)*(g * g - v)\n",
    "\n",
    "            m_corrected = m/(1 - beta1**t)\n",
    "            v_corrected = v/(1 - beta2**t)\n",
    "\n",
    "            m_values_t_1.append(m)\n",
    "            v_values_t_1.append(v)\n",
    "            param_deltas.append( alpha * m_corrected / (np.sqrt(v_corrected) + eps))\n",
    "\n",
    "        self.m_values[self.current_id] = m_values_t_1\n",
    "        self.v_values[self.current_id] = v_values_t_1\n",
    "        self.t_values[self.current_id] = t\n",
    "\n",
    "        return param_deltas\n",
    "        \n",
    "    def initialize(self, gradients):\n",
    "        self.m_values[self.current_id] = [np.zeros_like(gradient) for gradient in gradients]\n",
    "        self.v_values[self.current_id] = [np.zeros_like(gradient) for gradient in gradients]\n",
    "        self.t_values[self.current_id] = 0\n",
    "        self._initialized.add(self.current_id)\n",
    "\n",
    "    @property\n",
    "    def initialized(self):\n",
    "        return self.current_id in self._initialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your Adam implemenation is correct you should be able to achieve about .99 **test** accuracy on **MNIST** after the **first** or **second** epoch (with a learning rate of 0.001).\n",
    "Edit the code cell in the section \"Running the Training\" to test this (look for the lines with comments containing `# BONUS TASK ADAM`).\n",
    "\n",
    "If your implementation is correct you can gain 2 bonus points for completing our [bonus quiz](https://learn.ki-campus.org/courses/edgeai-hpi2022/items/7GjtyHsBaGNtWNVbbvPqxa) of this week.\n",
    "Before you attempt the quiz, you should train an MLP on **FashionMNIST** with Adam with the same settings (you need to uncomment another line in the training code to change the dataset - keep the learning rate of 0.001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Adam implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T09:35:25.691233Z",
     "iopub.status.busy": "2022-01-19T09:35:25.690876Z",
     "iopub.status.idle": "2022-01-19T09:37:08.830551Z",
     "shell.execute_reply": "2022-01-19T09:37:08.830021Z",
     "shell.execute_reply.started": "2022-01-19T09:35:25.691204Z"
    }
   },
   "outputs": [],
   "source": [
    "options = TrainOptions()\n",
    "options.optimizer=\"adam\"\n",
    "options.learning_rate = 0.001\n",
    "\n",
    "print('Training on MNIST')\n",
    "main(options)\n",
    "print('Training finished.')\n",
    "\n",
    "print('\\n\\n')\n",
    "print('Training on Fashion')\n",
    "options.data_set = \"fashion\"\n",
    "main(options)\n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The four proposed tasks\n",
    "\n",
    "### PT1. Changing the learning rate\n",
    "\n",
    "We will verify the influence of the learning rate on the model with SGD optimizer by changing its value and observing the accuracy of the model during the training process. The choosen values for the learning rate were 10.0, 5.0, 1.0, 0.01, 0.001, and 0.0001. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T09:37:08.832005Z",
     "iopub.status.busy": "2022-01-19T09:37:08.831641Z",
     "iopub.status.idle": "2022-01-19T09:40:25.038441Z",
     "shell.execute_reply": "2022-01-19T09:40:25.037827Z",
     "shell.execute_reply.started": "2022-01-19T09:37:08.831955Z"
    }
   },
   "outputs": [],
   "source": [
    "for lr in [10.0, 5.0, 1.0, 0.01, 0.001, 0.0001]:\n",
    "    print('Learning rate: {}'.format(lr))\n",
    "    print('\\n')\n",
    "    options = TrainOptions()\n",
    "    \n",
    "    options.batchsize = 128\n",
    "    options.learning_rate = lr\n",
    "\n",
    "    main(options)\n",
    "    print('Training finished')\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already know, the learning rate determines the step size that the optimizer is allowed to take in the direction given by the gradient vector. Given the results above, one may observe that, in this case, the learning rate may not be so high nor so small. Having the learning rate as big as 10 made the process take 2 epochs to achieve a precision of 0.8 (in this case, I thought it would diverge). On the other hand, reducing the learning rate resulted in a slower convergence process. It became so slow in the last three cases, that the model didn't achieved an accuracy higher than 0.7 - the last one achieved a poor accuracy of 0.15. In fact, it barely moved from the accuracy obtained in the first epoch. \n",
    "\n",
    "For two values, 5.0 and 1.0, the accuracy fastly converges. In the first epoch, it already achieves values equal or higher than 0.95, for both training and testing. It means that in just 900 iteractions, we already have a great model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PT2. Train a model for the FashionMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T09:40:25.043744Z",
     "iopub.status.busy": "2022-01-19T09:40:25.041823Z",
     "iopub.status.idle": "2022-01-19T09:40:57.364742Z",
     "shell.execute_reply": "2022-01-19T09:40:57.364223Z",
     "shell.execute_reply.started": "2022-01-19T09:40:25.043705Z"
    }
   },
   "outputs": [],
   "source": [
    "options = TrainOptions()\n",
    "options.dataset = 'fashion'\n",
    "\n",
    "main(options)\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PT3. Train for a few more epochs and see if you can achieve a higher accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T09:40:57.366352Z",
     "iopub.status.busy": "2022-01-19T09:40:57.365998Z",
     "iopub.status.idle": "2022-01-19T09:42:01.179725Z",
     "shell.execute_reply": "2022-01-19T09:42:01.179120Z",
     "shell.execute_reply.started": "2022-01-19T09:40:57.366320Z"
    }
   },
   "outputs": [],
   "source": [
    "options = TrainOptions()\n",
    "options.num_epochs = 10\n",
    "main(options)\n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PT4. Try a different learning rate update rule in the training loop implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T09:42:01.183858Z",
     "iopub.status.busy": "2022-01-19T09:42:01.183116Z",
     "iopub.status.idle": "2022-01-19T09:42:01.198830Z",
     "shell.execute_reply": "2022-01-19T09:42:01.198229Z",
     "shell.execute_reply.started": "2022-01-19T09:42:01.183824Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop_nuplr(args, data_set, model, optimizer):\n",
    "    # Training loop with new update rule for learning rate\n",
    "    for epoch in range(args.num_epochs):\n",
    "        # train our model\n",
    "        for iteration, batch in enumerate(data_set.train):\n",
    "            model.forward(batch)\n",
    "            model.backward(optimizer)\n",
    "\n",
    "            if iteration % args.train_verbosity == 0:\n",
    "                accuracy = F.accuracy(model.predictions, batch.labels).data\n",
    "                print(\"train: epoch: {: 2d}, loss: {: 5.2f}, accuracy {:.2f}, iteration: {: 4d}\".\n",
    "                      format(epoch, model.loss.data, accuracy, iteration), end=\"\\r\")\n",
    "        \n",
    "        # test our model\n",
    "        print(\"\\nrunning test set...\")\n",
    "        sum_accuracy = 0.0\n",
    "        sum_loss = 0.0\n",
    "        for iterations, batch in enumerate(data_set.test):\n",
    "            model.forward(batch, train=False)\n",
    "            sum_accuracy += F.accuracy(model.predictions, batch.labels).data\n",
    "            sum_loss += model.loss.data\n",
    "        nr_batches = iterations - 1\n",
    "        print(\" test: epoch: {: 2d}, loss: {: 5.2f}, accuracy {:.2f}\".\n",
    "              format(epoch, sum_loss / nr_batches, sum_accuracy / nr_batches))\n",
    "        \n",
    "        # Here we change the update rule for the learning rate. In this case, we just set the epoch when the lr \n",
    "        # is reduced to 10% of the previous value.\n",
    "        if epoch % 5 == 0:\n",
    "            optimizer.learning_rate *= 0.1\n",
    "\n",
    "            \n",
    "def main_new_update_rule_lr(args):\n",
    "    args.check()\n",
    "\n",
    "    data_set = None\n",
    "    if args.data_set == \"mnist\":\n",
    "        data_set = MNIST(args.batch_size)\n",
    "    if args.data_set == \"fashion\":\n",
    "        data_set = FashionMNIST(args.batch_size)\n",
    "\n",
    "    optimizer = None\n",
    "    if args.optimizer == \"adam\":\n",
    "        # BONUS TASK ADAM: if you implemented Adam below (and ran that code cell), then remove or comment the next line...\n",
    "        raise NotImplementedError(\"Adam not implemented yet.\")\n",
    "        # ... and uncomment the following line:\n",
    "        #optimizer = Adam(args.learning_rate)\n",
    "    if args.optimizer == \"sgd\":\n",
    "        optimizer = SGD(args.learning_rate)\n",
    "\n",
    "    model = ActivatedMLP()\n",
    "\n",
    "    train_loop_nuplr(args, data_set, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T09:42:01.203841Z",
     "iopub.status.busy": "2022-01-19T09:42:01.202306Z",
     "iopub.status.idle": "2022-01-19T09:43:04.191777Z",
     "shell.execute_reply": "2022-01-19T09:43:04.191184Z",
     "shell.execute_reply.started": "2022-01-19T09:42:01.203805Z"
    }
   },
   "outputs": [],
   "source": [
    "options = TrainOptions()\n",
    "options.num_epochs = 10\n",
    "main_new_update_rule_lr(options)\n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one may observe, the update rule has an impact on the model obtained. Since the update rule is a way to balance the advantages and disadvantages of having a large (or little) learning rate, it expected that changing this rule reflects on the convergence process of training the model. In the case above, were we just changed the epoch when the lr is reduced, we see that the final accuracy is slighlty higher than the one obtained in the **PT3**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
