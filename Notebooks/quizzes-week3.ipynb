{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66993cff",
   "metadata": {},
   "source": [
    "# Week 3 - Compact Network Design and Knowledge Distillation\n",
    "\n",
    "In this week, you will learn all about the design of compact neural networks and knowledge distillation. Both methods are of high importance when applying deep neural networks on edge devices because they reduce the memory, computation, and energy requirements of neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bec1076",
   "metadata": {},
   "source": [
    "# 1. Notations\n",
    "\n",
    "## 1.1 Neural Network Compression\n",
    "\n",
    "## 1.2 Compact Network Design (1/3)\n",
    "\n",
    "1. SqueezeNet\n",
    "- Fire module:\n",
    "    - Strategy: Design CNN Architecture with few parameters while maintening competitive accuracy\n",
    "    - Replace 3x3 filters with 1x1 filters (9x fewer parameters)\n",
    "    - Decrease number of inputs channels to 3x3 filters\n",
    "    - Downsample late in the network so that convolution layers have large activation maps: Our intuition is that large activation maps (due to delayed downsampling - for example, stride > 1) can lead to higher classification accuracy. This strategy is about maximazing accuracy on limited bugdet of parameters.\n",
    "    \n",
    "- ReLU is applied to activation from squeeze and expand layers\n",
    "- Dropout with 50% applied after fire9\n",
    "- Note the lack of fully-connected layers\n",
    "\n",
    "- Although the size of the model is smaller, the computation complexity was ignored\n",
    "\n",
    "2. Xception Network\n",
    "\n",
    "- Introduces the concept of Depthwise Separable Convolution (Section 14.4.3 of Probability ML (Murphy))\n",
    "- Number of groups = Number of channels -> results are concatenated after a 1x1 convolution\n",
    "- Maximizes the decoupling of spatial correlation and channel correlation\n",
    "\n",
    "## 1.3 Compact Network Design (2/3)\n",
    "\n",
    "1. MobileNet V1\n",
    "- Description of two model shrinking hyperparameters (unified network-dimension scaling mechanism)\n",
    "    - Width multiplier (thinner models)\n",
    "    - Resolution multiplier (reduced representation)\n",
    "- Depthwise Separable Convolution\n",
    "    - Splits convolutions into two layers: filtering and combining\n",
    "    - Effect: drastically reducing computation and model size\n",
    "- ReLU6 instead of ReLU (L2 regularizer)\n",
    "- Concentrates almost all computation into dense 1x1 convolution layers\n",
    "\n",
    "2. MobileNet V2\n",
    "- Significant decrease of number of operations and memory need\n",
    "- Novel layer module: the inverted residual with linear bottleneck\n",
    "- Inverted bottleneck design:\n",
    "    - Linear bottleneck:\n",
    "        - Attempt to preserve the information of the manifold of interest\n",
    "        - Is crucial to preserve non-linearities from destroying too much information\n",
    "        - Ratio between size of input bottleneck and inner size referred as expansion ratio.\n",
    "    - Inverted residuals\n",
    "        - Improve the ability of a gradient to propagate across multiplier layers\n",
    "        - More memory efficient\n",
    "\n",
    "3. MobileNetV3\n",
    "- Starts the exploration of how automated search algorithms and network design can work together \n",
    "- Combination of layers used in MobNet V1 and V2 as building blocks\n",
    "- Layers are also upgraded with modified swish nonlinearities (sigmoid replaced by hard sigmoid to maintain accuracy)\n",
    "- Use platform-aware NAS to search global network structure\n",
    "- Use NetAdapt algorithm to search per layer for number of filters\n",
    "\n",
    "## 1.4 Compact Network Design (3/3)\n",
    "\n",
    "1. Efficient Net\n",
    "- Carefully balancing network depth, width, and resolution can lead to better performance\n",
    "- New scalling method that uniformlly scales all dimensions of depth/width/resolution using simple compound coefficient (compound scaling method)\n",
    "- Baseline network developed by leveraging a multi-objective neural architecture search that optimizes accuracy and FLOPS\n",
    "- Main building block is mobile inverted bottleneck MBConv\n",
    "\n",
    "\n",
    "2. ShuffleNet V2\n",
    "- Proposes two principles for effective network architecture design\n",
    "    - The direct metric (e.g. speed) should be used instead of indirect ones (e.g. FLOPS)\n",
    "    - Such metric should be evaluated on the target platform\n",
    "- ShuffleNet V1 Architecture\n",
    "    - Pointwise group convolution\n",
    "        - Increase MAC (Memory Access Cost)\n",
    "    - Bottleneck-like structures\n",
    "        - Increase MAC\n",
    "    - Channel shuffle operation: enables information communication between different groups of channels\n",
    "        - Occupy considerable amount of time, specially on GPU\n",
    "- Channel Split and Shuffle Net V2:\n",
    "    - Channels are split into two branches at the beginning of each unit\n",
    "    - The add operation in ShuffleNet v1 no longer exists\n",
    "    - Depthwise convolutions exist only in one branch\n",
    "\n",
    "3. GhostNet\n",
    "\n",
    "4. AsymmNet\n",
    "- Based on proposed asymmetrical bottleneck design\n",
    "- Follow the basic network architecture of MobileNetV3\n",
    "- The main building blocks of AsymmNet consists of a sequence of stacked assymetrical bottleneck blocks\n",
    "    - Gradually downsample the feature map resolution and increase the channel number\n",
    "    \n",
    "\n",
    "5. RepVGG Net\n",
    "\n",
    "\n",
    "\n",
    "## 1.5 Knowledge Destillation (1/2)\n",
    "\n",
    "- Accordingly to the quizz question, energy consumption is reduced due to the fact that KD make it necessary only to deploy the student model\n",
    "\n",
    "## 1.6 Knowledge Destillation (2/2)\n",
    "\n",
    "- Loss function (distillation loss): Kullback-Leibler divergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7432ee",
   "metadata": {},
   "source": [
    "# 2. Quizzes \n",
    "\n",
    "## 2.1 Neural Network Compression\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "Why do we have to run models such as ResNet-152 on powerful servers in the cloud?\n",
    "\n",
    "- [ ] Edge devices are typically not allowed to draw the amount of current necessary to run deep neural networks\n",
    "    **Current is not a problem since with less processing power, we would only need to wait longer**\n",
    "- [x] Small edge devices are not powerful enough to compute 11.3 billion floating point operations with multiple frames per second **True**\n",
    "- [ ] Small edge devices typically use a different instruction set that can not be used to run deep neural networks **Not true**\n",
    "- [x] The storage needs can not be fulfilled by small edge devices **Yep**\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Which of the following methods can be used to squeeze deep models?\n",
    "\n",
    "- [x] model quantization/binarization\n",
    "- [ ] model parallelization\n",
    "- [x] knowledge distillation\n",
    "- [x] model pruning\n",
    "\n",
    "## 2.2 Compact Network Design (1/3)\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "What is the name of the block architecture introduced by SqueezeNet?\n",
    "\n",
    "- [ ] Ice \n",
    "- [ ] Water \n",
    "- [ ] Soil\n",
    "- [x] Fire\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Why is SqueezeNet architecture important?\n",
    "\n",
    "- [x] It was one of the first models that achieved AlexNet-level accuracy with 50x fewer parameters and less than 0.5MB model size\n",
    "- [ ] It was one of the first network architectures that solely consisted of 1x1 convolutions, which have less learnable parameters and use less storage. Thus, SqueezeNet was able to achieve AlexNet accuracy with a model size of less than 0.25MB\n",
    "- [ ] It was one of the first functioning image classification models with less than 0.5MB model size \n",
    "\n",
    "**Question 3**\n",
    "\n",
    "How many groups $g$ does a group convolution used as depthwise separable convolution have if the number of channels is defined as $c$?\n",
    "\n",
    "- [x] g = c\n",
    "- [ ] it doesn't matter\n",
    "- [ ] g = 2c\n",
    "- [ ] g = c/2 \n",
    "\n",
    "## 2.3 Compact Network Design (2/3)\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "Which of these are important building blocks of MobileNet V1?\n",
    "\n",
    "- [x] ReLu6\n",
    "- [ ] nearly all computations happens in 3x3 convolutions\n",
    "- [x] very little or no weight decay (**on depthwise filters**)\n",
    "- [ ] usage of residual connections\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Which of the following convolution functions and concepts are not required for a 1x1 convolution?\n",
    "\n",
    "- [ ] memory optimizations\n",
    "- [x] im2col\n",
    "- [ ] GEMM \n",
    "\n",
    "**Question 3**\n",
    "\n",
    "MobileNetv1 continues the leading positon in DNN backbone design of which company?\n",
    "\n",
    "- [ ] Amazon\n",
    "- [ ] Facebook/Meta\n",
    "- [ ] Alibaba\n",
    "- [x] Google \n",
    "\n",
    "**Question 4**\n",
    "\n",
    "Which additional activation function is used in MobileNetV3?\n",
    "\n",
    "- [ ] GELU\n",
    "- [ ] Mish\n",
    "- [ ] PReLU\n",
    "- [x] h-swish\n",
    "\n",
    "## 2.4 Compact Network Design (3/3)\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "Which of the following parameters can be scaled in EfficientNet?\n",
    "\n",
    "- [ ] Height\n",
    "- [x] Resolution\n",
    "- [x] Depth\n",
    "- [x] Width\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Given a description of the following characteristcs, which network architecture are we describing? \"Feature reuse at inverted bottleneck blocks by copying old features and computing new features. Enhancing the expressiveness by using depth-wise layers to extend the width.\"\n",
    "\n",
    "- [ ] EfficientNet\n",
    "- [ ] ShuffleNetv2\n",
    "- [x] AsymmNet\n",
    "- [ ] GhostNet \n",
    "\n",
    "**Question 3**\n",
    "\n",
    "Which techniques are the main characteristics of RepVGG?\n",
    "\n",
    "- [ ] channel shuffling\n",
    "- [ ] shortcut connections\n",
    "- [x] over parameterization\n",
    "- [x] linear combination \n",
    "\n",
    "**Question 4**\n",
    "\n",
    "Which of these convolution kernel sizes is the most friendly one for accelerators?\n",
    "\n",
    "- [ ] 9x9\n",
    "- [ ] 5x5\n",
    "- [x] 3x3\n",
    "- [ ] 7x7 \n",
    "\n",
    "\n",
    "## 2.5 Knowledge Destillation (1/2)\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "Which of the following statements about knowledge distillation are true?\n",
    "\n",
    "- [x] Knowledge distillation can be used to compress the knowledge of an ensemble of networks into a student network\n",
    "- [ ] Knowledge Distillation always involves the usage of multiple teacher models\n",
    "- [x] Knowledge distillation is a form of model compression where the knowledge of a large teacher model is compressed into a smaller student model\n",
    "- [ ] Knowledge distillation is the process of scaling the knowledge of a small teacher network to a larger student network \n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Why do we use soft labels instead of hard labels for knowledge distillation?\n",
    "\n",
    "- [ ] soft labels provide more sparse information about the label, leading to more confident predictions\n",
    "- [ ] soft labels are simpler to annotate, leading to larger training datasets\n",
    "- [x] soft labels have less gradient variance, leading to smoother training\n",
    "- [ ] soft labels have a lower entropy, thus allowing the model to learn from a stronger supervision. \n",
    "\n",
    "**Question 3**\n",
    "\n",
    "Why can knowledge distillation be used to reduce the energy consumption of AI computing?\n",
    "\n",
    "- [x] the models train faster\n",
    "- [x] it is only necessary to deploy the student model\n",
    "- [ ] the student model is automatically quantized, thus requiring less memory\n",
    "- [ ] the overall training process for the models requires less energy \n",
    "\n",
    "## 2.6 Knowledge Destillation (2/2)\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "When do we set a higher temperature for knowledge distillation?\n",
    "\n",
    "- [ ] If we have a model with fewer parameters\n",
    "- [ ] If we want to avoid being affected by noise in negative labels\n",
    "- [x] If we want to learn from negative labels that are informative\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "What kind of knowledge can be distilled?\n",
    "\n",
    "- [ ] Inheritance based knowledge\n",
    "- [x] Feature based knowledge\n",
    "- [x] Prediction based knowledge\n",
    "- [ ] Weights based knowledge\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "Which loss function is used to distill feature based knowledge?\n",
    "\n",
    "- [ ] Soft Cross Entropy\n",
    "- [x] Mean Squared Error\n",
    "- [ ] Kullback-Leibler divergence\n",
    "- [ ] Mean Absolute Error\n",
    "\n",
    "**Question 4**\n",
    "\n",
    "Which distillation scheme is especially effective if the training labels are noisy?\n",
    "\n",
    "- [ ] Offline distillation\n",
    "- [ ] Online distillation\n",
    "- [ ] Multi-Teacher distillation\n",
    "- [x] Self distillation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
