{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Knowledge Distillation on CIFAR-100 using PyTorch\n",
    "\n",
    "This is the third practical exercise of our course [Applied Edge AI](https://learn.ki-campus.org/courses/edgeai-hpi2022).\n",
    "In the last exercise, we trained a neural network for image classification on CIFAR-100 using PyTorch.\n",
    "In this exercise, we want to use the network we trained in the last exercise and distill the knowledge of that network into a smaller network.\n",
    "\n",
    "Similarly to the previous exercise, we provide you with a notebook with missing code sections.\n",
    "In the graded quiz at the end of the week, we might ask some questions that deal with this exercise, so make sure to do the exercise (and have your output handy) **before** taking the quiz!\n",
    "\n",
    "# Reusing Code\n",
    "\n",
    "In the last exercise, we wrote quite some code that can be reused here.\n",
    "We already added all of this code in the following cells.\n",
    "There is nothing you need to do, since you already wrote such code in the last exercise.\n",
    "\n",
    "We start with the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T14:58:54.457140Z",
     "iopub.status.busy": "2022-01-26T14:58:54.456240Z",
     "iopub.status.idle": "2022-01-26T14:58:58.189479Z",
     "shell.execute_reply": "2022-01-26T14:58:58.188761Z",
     "shell.execute_reply.started": "2022-01-26T14:58:54.457026Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import statistics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as tt\n",
    "import imgaug\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Type, List, Union\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler, OneCycleLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import cifar100_resnets as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we added the code for data loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T14:58:58.192121Z",
     "iopub.status.busy": "2022-01-26T14:58:58.191612Z",
     "iopub.status.idle": "2022-01-26T14:59:00.019482Z",
     "shell.execute_reply": "2022-01-26T14:59:00.018740Z",
     "shell.execute_reply.started": "2022-01-26T14:58:58.192084Z"
    }
   },
   "outputs": [],
   "source": [
    "class CIFAR100(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path: Path, image_transforms: tt.Compose, image_augmentations: Union[None, Type[iaa.Augmenter]] = None):\n",
    "        super().__init__()\n",
    "        data = pickle.load(dataset_path.open(\"rb\"), encoding=\"bytes\")\n",
    "        self.images = data[b\"data\"]\n",
    "        self.labels = data[b\"fine_labels\"]\n",
    "        \n",
    "        self.image_transforms = image_transforms\n",
    "        self.image_augmentations = image_augmentations\n",
    "        \n",
    "        assert len(self.images) == len(self.labels), \"Number of images and labels is not equal!\"\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> tuple:\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        image = np.reshape(image, (3, 32, 32))\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "        \n",
    "        if self.image_augmentations is not None:\n",
    "            image = self.image_augmentations.augment_image(image)\n",
    "        image = self.image_transforms(Image.fromarray(image))\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "image_transformations = tt.Compose([\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize(\n",
    "        mean=(0.5074, 0.4867, 0.4411),\n",
    "        std=(0.2011, 0.1987, 0.2025)\n",
    "    )\n",
    "])\n",
    "\n",
    "train_augmentations = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.CropAndPad(px=(-4, 4), pad_mode=\"reflect\")\n",
    "])\n",
    "\n",
    "\n",
    "class CIFAR100Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_type: str = \"resnet18\", temperature: int = 1):\n",
    "        super().__init__()\n",
    "        model_class = getattr(models, model_type)\n",
    "        self.feature_extractor = model_class(num_classes=100)\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        activations = self.feature_extractor(images)\n",
    "        return activations / self.temperature\n",
    "\n",
    "    \n",
    "def accuracy(predictions: torch.Tensor, labels: torch.Tensor, reduce_mean: bool = True) -> torch.Tensor:\n",
    "    predicted_classes = torch.argmax(F.softmax(predictions, dim=1), dim=1)\n",
    "    correct_predictions = torch.sum(predicted_classes == labels)\n",
    "    if reduce_mean:\n",
    "        return correct_predictions / len(labels)\n",
    "    return correct_predictions\n",
    "\n",
    "\n",
    "def test_model(network: Type[nn.Module], data_loader: DataLoader) -> float:\n",
    "    num_correct_predictions = 0\n",
    "    device = get_device()\n",
    "    \n",
    "    for images, labels in data_loader:\n",
    "        images = to_device(images, device)\n",
    "        labels = to_device(labels, device)\n",
    "        predictions = network(images)\n",
    "        num_correct_predictions += float(accuracy(predictions, labels, reduce_mean=False).item())\n",
    "        \n",
    "    return num_correct_predictions / len(data_loader.dataset)\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def to_device(data: torch.Tensor, device: torch.device) -> torch.Tensor:\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device)\n",
    "\n",
    "\n",
    "def plot_metrics(metrics: dict):\n",
    "    # we prepare the plotting by creating a set of axes for plotting, we want to put each metric in its own plot in a separate row\n",
    "    # furthermore, all plots should share the same x-axis values\n",
    "    fig, axes = plt.subplots(math.ceil(len(metrics) / 2), 2, sharex=True, figsize=(20, 20))\n",
    "\n",
    "    # we want to have a set of distinct colors for each logged metric\n",
    "    colors = iter(plt.cm.rainbow(np.linspace(0, 1, len(metrics))))\n",
    "    \n",
    "    # create the actual plot\n",
    "    for (metric_name, metric_values), axis in zip(metrics.items(), axes.flatten()):\n",
    "        iterations = []\n",
    "        values = []\n",
    "        for logged_value in metric_values:\n",
    "            iterations.append(logged_value[\"iteration\"])\n",
    "            values.append(logged_value[\"value\"])\n",
    "        axis.plot(iterations, values, label=metric_name, color=next(colors))\n",
    "        axis.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_dataset = CIFAR100(Path(\"/kaggle/input/cifar100/train\"), image_transformations, train_augmentations)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = CIFAR100(Path(\"/kaggle/input/cifar100/test\"), image_transformations)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation\n",
    "\n",
    "In knowledge distillation, our aim is to distill the knowledge of a large model into a smaller model.\n",
    "We can do this in two ways:\n",
    "1. Train the large and the small model at the same time. Here, we train the large model only on the hard labels provided by the dataset. We train the small model using the soft labels provided by the large model.\n",
    "1. Train the large model first, then train the small models based on the outputs of the large model.\n",
    "\n",
    "We will try to train both models at the same time.\n",
    "However, we highly encourage you to also try the other way, where we first train a larger model and then a smaller model!\n",
    "\n",
    "To run the training, we need to perform the following steps:\n",
    "1. build two networks (a large and a smaller one)\n",
    "1. adapt the training code from last week to use two networks and also perform the correct loss calculations\n",
    "\n",
    "# Task 1: Building the Networks\n",
    "\n",
    "We will start with the creation of the networks, which should be fairly simple.\n",
    "Have a look at the `CIFAR100Net` class above and figure out how you can use that class to build a `resnet56` and a `resnet20` model.\n",
    "Note that the variable `models` was imported in our first code cell, from the included [utility scripts](https://www.kaggle.com/bartzi/cifar100-resnets).\n",
    "We will use the `resnet56` model as the teacher model and the `resnet20` model as the student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2022-01-26T14:59:00.022682Z",
     "iopub.status.busy": "2022-01-26T14:59:00.022493Z",
     "iopub.status.idle": "2022-01-26T14:59:00.098663Z",
     "shell.execute_reply": "2022-01-26T14:59:00.097992Z",
     "shell.execute_reply.started": "2022-01-26T14:59:00.022657Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbf266aa3f73473cfc5179af5e120cec",
     "grade": true,
     "grade_id": "cell-cfda06aeb80ffa39",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# define `teacher_model` as a ResNet with 56 layers based on CIFAR100Net\n",
    "# define `student_model` as a ResNet with 20 layers based on CIFAR100Net\n",
    "teacher_model = CIFAR100Net(model_type='resnet56', temperature=1)\n",
    "student_model = CIFAR100Net(model_type='resnet20', temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the networks was simple.\n",
    "Now, we need to adapt the training loop from last week.\n",
    "\n",
    "# Task 2: Adapt our Training Code\n",
    "\n",
    "You can reuse most parts of the training loop but we have to make the following changes:\n",
    "\n",
    "1. Since we are now handling two networks at the same time, we have to adopt our code to use two networks and also two optimizers (they should be given as parameters to the `train` function).\n",
    "1. We have to adapt our `train_for_one_iteration` function. Here, we need to forward the batch through both networks, then we calculate the losses:\n",
    "  1. the loss for the teacher network using the hard labels\n",
    "  1. the loss for the student network using the soft labels (the kullback leibler divergence or cross entropy of the softmax outputs of both networks) + the hard labels ([HINT](https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html): make sure to disallow the flow of gradients to the teacher network when using the softmax outputs of the teacher network)\n",
    "1. following the loss calculations, we need to run the backward passes for both networks and run the weight updates using the optimizers\n",
    "1. we can then return the losses of both networks\n",
    "\n",
    "## Task 2a: Initialize the Loss Functions\n",
    "\n",
    "First we should initialize our two loss functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2022-01-26T14:59:00.101163Z",
     "iopub.status.busy": "2022-01-26T14:59:00.100882Z",
     "iopub.status.idle": "2022-01-26T14:59:00.108336Z",
     "shell.execute_reply": "2022-01-26T14:59:00.106527Z",
     "shell.execute_reply.started": "2022-01-26T14:59:00.101127Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dcb94097a7d4352fbebaa91fe7f85743",
     "grade": true,
     "grade_id": "cell-33ce1080138595ef",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: initialize the correct loss functions\n",
    "# 1. teacher_loss_function should contain a PyTorch function for the Cross Entropy loss\n",
    "# 2. a) student_loss_function should contain a PyTorch implementation of the Kullback-Leibler divergence loss\n",
    "#    b) make sure, the mean of the student loss is calculated over the batch dimension only - not over all dimensions\n",
    "#    c) check out the documentation of the Kullback-Leibler divergence loss particularly about\n",
    "#       whether the inputs expect probabilities or log-probabilities\n",
    "teacher_loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "class student_loss_function(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kld_loss = nn.KLDivLoss(log_target=False, reduction='batchmean')\n",
    "        \n",
    "    def forward(self, student_output: Type[torch.Tensor], teacher_output: Type[torch.Tensor]) -> torch.Tensor:\n",
    "        \n",
    "        return self.kld_loss(student_output, teacher_output).sum(dim=0)\n",
    "    \n",
    "student_loss_function = student_loss_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T14:59:00.110436Z",
     "iopub.status.busy": "2022-01-26T14:59:00.109565Z",
     "iopub.status.idle": "2022-01-26T14:59:00.175571Z",
     "shell.execute_reply": "2022-01-26T14:59:00.174912Z",
     "shell.execute_reply.started": "2022-01-26T14:59:00.110401Z"
    }
   },
   "outputs": [],
   "source": [
    "## Testing KLDivLoss\n",
    "kld_loss = nn.KLDivLoss(log_target=False, reduction='none')\n",
    "#input = torch.randn(10, 2, 3)\n",
    "#output = torch.randn(10, 2, 3)\n",
    "\n",
    "input = torch.tensor([[1 , 1 ,1], [1 , 1 , 1]])\n",
    "output = torch.tensor([[1 , 1 ,1], [1 , 1 , 1]])\n",
    "\n",
    "print(input.shape)\n",
    "print(kld_loss(input,output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T14:54:24.120412Z",
     "iopub.status.busy": "2022-01-26T14:54:24.119462Z",
     "iopub.status.idle": "2022-01-26T14:54:28.588907Z",
     "shell.execute_reply": "2022-01-26T14:54:28.587745Z",
     "shell.execute_reply.started": "2022-01-26T14:54:24.120356Z"
    }
   },
   "source": [
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 2\n",
    "\n",
    "teacher_model = teacher_model.to(get_device())\n",
    "student_model = student_model.to(get_device())\n",
    "\n",
    "teacher_optimizer = torch.optim.Adam(teacher_model.parameters(), lr=learning_rate)\n",
    "student_optimizer = torch.optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "\n",
    "for idx, batch in enumerate(train_data_loader): \n",
    "\n",
    "    print('Target', batch[1])\n",
    "    res_dict = train_for_one_iteration(\n",
    "                networks=[teacher_model, student_model],\n",
    "                batch = batch,\n",
    "                optimizers = [teacher_optimizer, student_optimizer],\n",
    "            )\n",
    "    \n",
    "    print(res_dict)\n",
    "    \n",
    "    raise '!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b: Adapt the Training Logic\n",
    "\n",
    "Then we can adapt our training logic for a single batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2022-01-26T14:59:00.176792Z",
     "iopub.status.busy": "2022-01-26T14:59:00.176576Z",
     "iopub.status.idle": "2022-01-26T14:59:00.186005Z",
     "shell.execute_reply": "2022-01-26T14:59:00.185131Z",
     "shell.execute_reply.started": "2022-01-26T14:59:00.176761Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d27523c8704fa58d4b1917369071f71a",
     "grade": true,
     "grade_id": "cell-665f161151df2202",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_for_one_iteration(networks: List[Type[nn.Module]], batch: tuple, optimizers: List[Type[Optimizer]]) -> dict:\n",
    "    images, labels = batch\n",
    "    teacher_network, student_network = networks\n",
    "\n",
    "    # TODO: do the forward pass and loss calculation for the *teacher* network:\n",
    "    # 1. pass the images through the `teacher_network`, store the result (the predictions) in `teacher_predictions`\n",
    "    # 2. calculate the `teacher_loss` with the `teacher_loss_function` based on the `teacher_predictions` and the labels\n",
    "    teacher_predictions = teacher_network(images)\n",
    "    teacher_loss = teacher_loss_function(teacher_predictions, labels)\n",
    "    \n",
    "    # TODO: do the forward pass and loss calculation for the *student* network:\n",
    "    # 1. pass the images through the `student_network`, store the result (the predictions) in `student_predictions`\n",
    "    # 2. calculate the cross entropy loss `student_ce_loss` with the `teacher_loss_function` based on the `student_predictions` and the labels\n",
    "    # 3. calculate the knowledge distillaion loss `student_kd_loss` based on:\n",
    "    #    1) the softmax of our `student_predictions` calculated on the label axis (dim 1)\n",
    "    #    2) the softmax of our `teacher_predictions` calculated on the label axis (dim 1)\n",
    "    #    HINT: check whether you need to include the regular or the logarithmic softmax for each one (refer to the documentation of the loss function)\n",
    "    # 4. disable gradient calculation of the teacher in the previous step:\n",
    "    #    add `.detach()` to `teacher_predictions`, which forwards the outputs but disables backpropagation\n",
    "    # 5. add up both losses (`student_ce_loss` and `student_kd_loss`) as the `student_loss`\n",
    "    student_predictions = student_network(images)\n",
    "    student_ce_loss = teacher_loss_function(student_predictions, labels)\n",
    "    \n",
    "    softmax_function = nn.Softmax(dim=1)\n",
    "    sfmx_student_predictions = softmax_function(student_predictions)\n",
    "    sfmx_teacher_predictions = softmax_function(teacher_predictions).detach()\n",
    "    \n",
    "    student_kd_loss = student_loss_function(sfmx_student_predictions, sfmx_teacher_predictions)\n",
    "    \n",
    "    student_loss = student_ce_loss + student_kd_loss    \n",
    "    \n",
    "    # calculate the accuracy of both predictions\n",
    "    teacher_accuracy = accuracy(teacher_predictions, labels)\n",
    "    student_accuracy = accuracy(student_predictions, labels)\n",
    "        \n",
    "    # Here come the real weight adjustments, first zero gradients, then calculate derivatives, followed by the actual update of the optimizer\n",
    "    for loss, optimizer in zip([teacher_loss, student_loss], optimizers):\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return {\n",
    "        \"teacher_loss\": float(teacher_loss.item()),\n",
    "        \"teacher_train_acc\": float(teacher_accuracy.item()),\n",
    "        \"student_loss\": float(student_loss.item()),\n",
    "        \"student_train_acc\": float(student_accuracy.item()),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already adapted the final `train` function for training both the student and the teacher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T14:59:00.187921Z",
     "iopub.status.busy": "2022-01-26T14:59:00.187474Z",
     "iopub.status.idle": "2022-01-26T14:59:00.203870Z",
     "shell.execute_reply": "2022-01-26T14:59:00.202980Z",
     "shell.execute_reply.started": "2022-01-26T14:59:00.187887Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_data: DataLoader, test_data: DataLoader, networks: List[Type[nn.Module]], optimizers: List[Type[Optimizer]], \\\n",
    "          lr_schedulers: List[Type[_LRScheduler]], num_epochs: int, update_lr_scheduler_each_iteration: bool = True) -> dict:\n",
    "    device = get_device()\n",
    "    # we save all metrics that we want to plot later on\n",
    "    metrics = defaultdict(list)\n",
    "    \n",
    "    for epoch in trange(num_epochs, desc=\"Epoch: \"):\n",
    "        losses = defaultdict(list)\n",
    "        \n",
    "        with tqdm(total=len(train_data), desc=\"Iteration: \") as progress_bar:\n",
    "            for iteration, batch in enumerate(train_data):\n",
    "                current_iteration = epoch * len(train_data) + iteration\n",
    "                \n",
    "                batch = to_device(batch, device)\n",
    "                calculated_losses = train_for_one_iteration(networks, batch, optimizers)\n",
    "                \n",
    "                for loss_name, loss_value in calculated_losses.items():\n",
    "                    losses[loss_name].append(loss_value)\n",
    "                    metrics[loss_name].append({\"iteration\": current_iteration, \"value\": loss_value})\n",
    "                # postfix_data is used to display current metrics in the progress bar\n",
    "                postfix_data = {name: f\"{value:.2f}\" for name, value in calculated_losses.items()}\n",
    "                \n",
    "                current_learning_rate = lr_schedulers[0].get_last_lr()[0]\n",
    "                postfix_data[\"lr\"] = f\"{current_learning_rate:.6f}\"\n",
    "                metrics[\"lr\"].append({\"iteration\": current_iteration, \"value\": current_learning_rate})\n",
    "                \n",
    "                progress_bar.set_postfix(postfix_data)\n",
    "                progress_bar.update()\n",
    "                \n",
    "                if update_lr_scheduler_each_iteration:\n",
    "                    for scheduler in lr_schedulers:\n",
    "                        scheduler.step()\n",
    "\n",
    "            progress_bar.set_description_str(\"Testing: \")\n",
    "            accuracies = {}\n",
    "            for metric_name,network in zip([\"teacher_acc\", \"student_acc\"], networks):\n",
    "                accuracy = test_model(network, test_data)\n",
    "                accuracies[f\"{metric_name}\"] = f\"{accuracy:.2f}\"\n",
    "                metrics[metric_name].append({\"iteration\": (epoch + 1) * len(train_data), \"value\": accuracy})\n",
    "\n",
    "            progress_bar.set_description_str(f\"Epoch: {epoch}\")\n",
    "            postfix_data = {name: f\"{statistics.mean(loss):.2f}\" for name, loss in losses.items()}\n",
    "            postfix_data.update()\n",
    "            postfix_data.update(accuracies)\n",
    "            progress_bar.set_postfix(postfix_data)\n",
    "            progress_bar.update()\n",
    "            \n",
    "            if not update_lr_scheduler_each_iteration:\n",
    "                    for scheduler in lr_schedulers:\n",
    "                        scheduler.step()\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just need to perform the last setup steps and then start the training. \\O/\n",
    "\n",
    "Before starting the training below, you should enable the GPU acclerator in the sidebar on the right (you can open the sidebar by clicking on the |< Symbol in the top right, then select *Settings*, *Accelerator*, *GPU*).\n",
    "\n",
    "If you have not done so at the beginning of working on this exercise (which is fine), this means the other cells need to be run again.\n",
    "To do so, you can select *Run All* in the top toolbar.\n",
    "The notebook should run most of the previous cells very quickly until the training below is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T14:59:00.205512Z",
     "iopub.status.busy": "2022-01-26T14:59:00.205098Z",
     "iopub.status.idle": "2022-01-26T15:54:26.918935Z",
     "shell.execute_reply": "2022-01-26T15:54:26.918145Z",
     "shell.execute_reply.started": "2022-01-26T14:59:00.205475Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 50\n",
    "\n",
    "teacher_model = teacher_model.to(get_device())\n",
    "student_model = student_model.to(get_device())\n",
    "\n",
    "teacher_optimizer = torch.optim.Adam(teacher_model.parameters(), lr=learning_rate)\n",
    "student_optimizer = torch.optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_iterations = num_epochs * (len(train_dataset) / BATCH_SIZE)\n",
    "teacher_scheduler = OneCycleLR(teacher_optimizer, learning_rate, epochs=num_epochs, steps_per_epoch=len(train_data_loader))\n",
    "student_scheduler = OneCycleLR(student_optimizer, learning_rate, epochs=num_epochs, steps_per_epoch=len(train_data_loader))\n",
    "\n",
    "# we are done with all setup and can start the training\n",
    "logged_metrics = train(\n",
    "    train_data_loader,\n",
    "    test_data_loader,\n",
    "    [teacher_model, student_model],\n",
    "    [teacher_optimizer, student_optimizer],\n",
    "    [teacher_scheduler, student_scheduler],\n",
    "    num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting of Progress\n",
    "\n",
    "As in the last exercise, we can now plot the train progress using the `plot_metrics` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T15:54:26.921374Z",
     "iopub.status.busy": "2022-01-26T15:54:26.920879Z",
     "iopub.status.idle": "2022-01-26T15:54:28.097630Z",
     "shell.execute_reply": "2022-01-26T15:54:28.096772Z",
     "shell.execute_reply.started": "2022-01-26T15:54:26.921331Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_metrics(logged_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Now?\n",
    "\n",
    "Similar to the last week, you should keep in mind what you just did in this exercise, as we will ask about the implementation in the graded test.\n",
    "Since there was not too much coding required so far, we hope you are wondering, what else there is to do?\n",
    "So we have prepared some suggestions:\n",
    "\n",
    "You could also test different training optimizations and try to get the best performance out of your student model.\n",
    "If you already developed a few improvements in the last week, you should try to use it in this week as well.\n",
    "As you may have noticed we already include the learning rate scheduler [OneCycleLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html) in this week's code.\n",
    "Maybe you can try to find another scheduling that performs better or integrate improvements from the previous week?\n",
    "\n",
    "Another interesting experiment might be to compare the performance of the model trained this week (with Knowledge Distillation) and the model from last week.\n",
    "To do this, go back to the previous exercise and compare the accuracies.\n",
    "You can also adapt the code above and train the student model completely independent of the teacher model.\n",
    "To do so, you should first train the teacher individually, and then use it to train the student network.\n",
    "\n",
    "Now you can compare the accuracy to the ResNet-56 and the ResNet-20 that was achieved with simultaneous training.\n",
    "Which models are performing better and why?\n",
    "\n",
    "Another interesting question is, how much computation (during inference) you can save when you are using the ResNet-20 with distilled knowledge.\n",
    "To see this, we can calculate the number of operations of each network using the [torchinfo](https://github.com/TylerYep/torchinfo) package and the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T15:54:28.100671Z",
     "iopub.status.busy": "2022-01-26T15:54:28.100125Z",
     "iopub.status.idle": "2022-01-26T15:54:37.207785Z",
     "shell.execute_reply": "2022-01-26T15:54:37.206943Z",
     "shell.execute_reply.started": "2022-01-26T15:54:28.100606Z"
    }
   },
   "outputs": [],
   "source": [
    "# try to import the library we need for calculating the number of operations\n",
    "# (if we can not import it, we need to install it)\n",
    "try:\n",
    "    import torchinfo\n",
    "except ImportError:\n",
    "    !pip install torchinfo\n",
    "    import torchinfo\n",
    "# if you get the warning \"Failed to establish a new connection\", go to the side bar on the right, then \"Settings\" and switch on \"Internet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Model Summary\n",
    "\n",
    "After installing torchinfo, we can now print the summary of our teacher model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T15:54:37.211237Z",
     "iopub.status.busy": "2022-01-26T15:54:37.210638Z",
     "iopub.status.idle": "2022-01-26T15:54:37.269929Z",
     "shell.execute_reply": "2022-01-26T15:54:37.269094Z",
     "shell.execute_reply.started": "2022-01-26T15:54:37.211191Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "batch_size = 1\n",
    "print(summary(teacher_model, input_size=(batch_size, 3, 32, 32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Model Summary\n",
    "\n",
    "And compare that to our student model (which should be much smaller):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T15:54:37.271856Z",
     "iopub.status.busy": "2022-01-26T15:54:37.271591Z",
     "iopub.status.idle": "2022-01-26T15:54:37.295603Z",
     "shell.execute_reply": "2022-01-26T15:54:37.294926Z",
     "shell.execute_reply.started": "2022-01-26T15:54:37.271819Z"
    }
   },
   "outputs": [],
   "source": [
    "print(summary(student_model, input_size=(batch_size, 3, 32, 32)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
