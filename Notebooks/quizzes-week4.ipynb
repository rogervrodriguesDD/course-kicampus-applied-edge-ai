{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d0852a5",
   "metadata": {},
   "source": [
    "# Week 4 - Advanced Deep Compression Methods\n",
    "\n",
    "## 1. Notations\n",
    "\n",
    "### 1.1 Network Pruning\n",
    "\n",
    "1. Pruning method:\n",
    "    - Method for model compression\n",
    "    \n",
    "    - The effect of dropout is very similar to the pruning method\n",
    "    \n",
    "    - Basic pruning methods:\n",
    "        - Individual weight pruning (unstructured pruning method)\n",
    "            - Rank the weights using L1 norm\n",
    "            - Set the x% weight values to zero\n",
    "            \n",
    "        - Neuron (channel) pruning (structured pruning method)\n",
    "            - Rank the weight columns using L2 norm\n",
    "            - Set the x% weight columns to zero\n",
    "            - This is equivalent to delete the corresponding output neurons\n",
    "            \n",
    "    - Unstructured pruning\n",
    "        - Prunes individuals parameters\n",
    "        \n",
    "    - Structured pruning\n",
    "        - Consider parameters in groups removing entire neurons, filters or channels\n",
    "            \n",
    "    - Iterative pruning\n",
    "        - Avoid performance degradation\n",
    "        \n",
    "        - Disadvantages\n",
    "            - Iterative pruning and fine-tuning is time consuming\n",
    "\n",
    "### 1.2 Dynamic Network\n",
    "\n",
    "- Dynamic NN can adapt their structures or parameters to different inputs\n",
    "\n",
    "- Advantages:\n",
    "    - Efficiency (computational and data)\n",
    "    - Representation power (?)\n",
    "    - Adaptiveness (trade-off between accuracy and efficiency)\n",
    "    - Compatibility (applicable on most advanced techniques of DL)\n",
    "    - Generality (wide range of applications)\n",
    "    - Interpretability\n",
    "    \n",
    "- Three main categories:\n",
    "    - Sample-wise dynamic\n",
    "    - Spatial-wise dynamic\n",
    "    - Temporal-wise dynamic\n",
    "    \n",
    "- AutoSlim\n",
    "    - Slimmable network: \n",
    "\n",
    "### 1.3 Super-subnets Architecture \n",
    "\n",
    "1. Single Stage: BigNAS \n",
    "    - Train a single model that can be sliced w.r.t the without resource budget any post-processing\n",
    "    - A series of problems needed attention in order to achieve success in this work: Initialization, Regularization, Convergence Behavior, Batch Norm calibration.\n",
    "\n",
    "2. Single Path One-Shot\n",
    "\n",
    "3. Once-for-all\n",
    "\n",
    "### 1.4 Neural Network Quantization \n",
    "\n",
    "- Converting the weights from float to int. Example: FP32 -> int8 (Case study: post-training quantization)\n",
    "- Types of quantization methods:\n",
    "    - Post-training (static) quantization\n",
    "    - Quantization aware training\n",
    "    \n",
    "- Disadvantage: Quantization layers are not differentiable!\n",
    "    \n",
    "### 1.5 Binary Neural Network (1/2)\n",
    "\n",
    "- Benefits:\n",
    "    - Energy saving\n",
    "    - Smaller model\n",
    "    - Faster\n",
    "    \n",
    "- Activation function: sign(x)\n",
    "    - Does not help for backpropagation\n",
    "    - Straight Through Estimation is then used as derivative approximation\n",
    "    \n",
    "- BNN Inference:\n",
    "    Given a defined full precision model\n",
    "    1. Convert full precision weights\n",
    "    2. Binarize the input\n",
    "    3. Apply xnor, popcnt operations (convulution in binary world) **Why xnor?**\n",
    "\n",
    "### 1.6 Binary Neural Network (2/2)\n",
    "\n",
    "1. Binary Dense Net\n",
    "    - BNN\n",
    "        - Reduced computational costs\n",
    "            - Up to 32x memory saving\n",
    "            - 58x speedup\n",
    "        - Substantial accuracy degradation\n",
    "    - Improvement over XNORNet and Bi-Real Net\n",
    "    - Previous works didn't considered the specificity of the binary space in using the same architecture used in the float point (real value) space\n",
    "    - Divides the efforts for binarization and compression into three categories: \n",
    "        - Compact network design\n",
    "        - Networks with quantized weights\n",
    "        - Networks with quantized weights and activations\n",
    "            - BNN: efficient calculation methods for equivalent of matrix multiplication by using xnor and bicount operations\n",
    "            - XNOR-Net: introduced a channel-wise scalling factor to reduce the approximation error of full-precision parameters\n",
    "            - ABC-Nets: used multiple weight bases and activation bases to approximate their full-precision counterparts\n",
    "            - Bi-Real Net: applies an extremelly sophisticated training strategy (full-precision pre-training, multi-step initialization, and custom gradients)\n",
    "     \n",
    "     - Golden Rules for Architecture Design\n",
    "         1. Maintaining rich information flow of the network\n",
    "         2. Compact Network Design are not well suited for BNNs\n",
    "         3. Bottleneck design should be eliminated \n",
    "         4. Consider using full-precision downsampling\n",
    "         5. Using shortcut connections to avoid bottlenecks of information flow\n",
    "         6. To overcome bottlenecks of information flow, increase the network width\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f630a4a7",
   "metadata": {},
   "source": [
    "## 2. Quizzes \n",
    "\n",
    "### 2.1 Network Pruning\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "Which of the following statements about pruning are true?\n",
    "\n",
    "- [ ] Dropout has the same effect and purpose as pruning.\n",
    "- [x] Neural network pruning is inspired by synaptic pruning of biological neuron system.\n",
    "- [x] It utilizes the fact that large models are often overparameterized.\n",
    "- [x] Pruning is a method for model regularization. \n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Which of the following are commonly used pruning methods?\n",
    "\n",
    "- [x] Unstructured pruning\n",
    "- [x] Weight pruning\n",
    "- [x] Neuron pruning\n",
    "- [x] Structured pruning \n",
    "\n",
    "### 2.2 Dynamic Network \n",
    "\n",
    "**Question 1**\n",
    "\n",
    "Which of the following statements about dynamic neural networks are **wrong**?\n",
    "\n",
    "- [x] A dynamic neural network can have dynamic width, depth, and path, but it must rely on a pre-trained network.\n",
    "- [ ] A dynamic neural network learns a specific network structure for each sample.\n",
    "- [ ] The dynamic neural network applies a decision-making mechanism for structure prediction.\n",
    "- [x] A dynamic neural network has a fixed network architecture but adaptive weight intensities. \n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Dynamic neural network has advantages, because...\n",
    "\n",
    "- [ ] The human brains process information statically.\n",
    "- [x] Dynamic models are able to achieve a desired trade-off between accuracy and efficiency for dealing with varying computational budgets on the fly.\n",
    "- [x] It allocates resources on demand at test time.\n",
    "- [x] It enlarges parameter space and improves representation power. \n",
    "\n",
    "**Question 3**\n",
    "\n",
    "Which of the following statements about early exits are true?\n",
    "\n",
    "- [x] Adding early exits will slightly increase the training complexity and time. (**Why will it increase complexity and time?**)\n",
    "- [ ] Early exits is a dynamic width method.\n",
    "- [ ] We can maximally have three early exits in a dynamic neural network. \n",
    "\n",
    "\n",
    "### 2.3 Supernet-Subnets Architecture Search \n",
    "\n",
    "**Question 1**\n",
    "\n",
    "Supernet can be regarded as a teacher for all subnets, using knowledge destillation to train subnets.\n",
    "\n",
    "- [x] yes\n",
    "- [ ] no\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "What are the benefits when supernet shares weights with all subnets?\n",
    "\n",
    "- [x] It can be considered as a type of ensemble learning.\n",
    "- [x] It is significantly reducing the training complexity.\n",
    "- [ ] The trained supernet is high-precision and highly compact. \n",
    "\n",
    "**Question 3**\n",
    "\n",
    "BigNAS uses the sandwich rule to train N random sample models. \n",
    "The initial weights of a small sub-network are inherited from a well-trained larger sub-network. Is this statement correct?\n",
    "\n",
    "- [ ] yes\n",
    "- [x] no **The problem with initialization was resolved using initializing the output of the residual block**\n",
    "\n",
    "### 2.4 Neural Network Quantization\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "Generally speaking, the quantized neural network will have better accuracy because the information loss generated by quantization can have a specific regularization effect. Therefore, the lower the bit, the higher the accuracy. Is this correct?\n",
    "\n",
    "- [x] no\n",
    "- [ ] yes\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "What are the advantages of neural network quantization?\n",
    "\n",
    "- [ ] It can enrich the distribution range of neural networks and improve the expressiveness of the model.\n",
    "- [x] The quantized models can support more applications of low-power devices.\n",
    "- [x] It can save memory and improve inference speed. \n",
    "\n",
    "**Question 3**\n",
    "\n",
    "What are the differences between post-training quantization and quantization aware training?\n",
    "\n",
    "- [ ] Post-training quantization needs to retrain the model.\n",
    "- [x] Post-training quantization does not require retraining the model.\n",
    "- [x] Quantization-aware training will take the effect of information loss into account during training, and it can thus have minor sacrifices to the inference accuracies. \n",
    "\n",
    "### 2.5 Binary Neural Network (1/2)\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "Which of the following statements about binary neural networks are correct?\n",
    "\n",
    "- [x] A binary neural network uses 1-bit parameter for both weights and activations.\n",
    "- [ ] A binary neural network uses 8-bit weights and 1-bit activations.\n",
    "- [ ] A binary neural network uses 1-bit weights and 8-bit activations. \n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Which of the following statements are correct?\n",
    "\n",
    "- [x] Sign function binarizes weights and activations to +1 and -1.\n",
    "- [x] Binary neural networks apply more efficient bitwise operators, e.g., XNOR, bitcount instead of arithmetic operations.\n",
    "- [ ] Differentiating the sign function does not help backpropagation since we got gradient values \"1\" almost everywhere.\n",
    "- [ ] Straight Through Estimation (STE) is the derivative function of Sign function. \n",
    "\n",
    "### 2.6 Binary Neural Network (2/2)\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "Which of the following are not the challenges for BNN?\n",
    "\n",
    "- [ ] Lack of tailor-made optimizer for BNN\n",
    "- [ ] Balancing accuracy and energy consumption for AI accelerators\n",
    "- [ ] Loss of accuracy compared to 32-bit networks\n",
    "- [x] Lack of support from the community and industry \n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Which of the following are **not** the main contributions of Bi-Real Net?\n",
    "\n",
    "- [x] Progressively approximated sign function\n",
    "- [ ] Two-stage training method\n",
    "- [ ] Binary-Real valued information flow\n",
    "- [x] Channel wise scaling factor for activations and weights. \n",
    "\n",
    "**Question 3**\n",
    "\n",
    "What are the main problems of BNN optimization?\n",
    "\n",
    "- [x] SGD cannot be applied to binary weights.\n",
    "- [x] Gradient mismatching problem\n",
    "- [x] Unnecessary computation and memory cost caused by full-precision latent weights.\n",
    "- [ ] We cannot use full-precision latent weights for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a43a42d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
