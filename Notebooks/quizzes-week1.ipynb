{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18726919",
   "metadata": {},
   "source": [
    "# Week 1 - Basics and History of Neural Networks\n",
    "\n",
    "## Quizzes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a672b",
   "metadata": {},
   "source": [
    "### History of NN 1\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "Which of the following words describe parts of a biological neuron?\n",
    "\n",
    "-  [ ] Corticus\n",
    "- [x] Soma\n",
    "- [x] Dendrite\n",
    "- [x] Synapse\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Which feature of the mammalian visual cortex is an important observation / enlightenment for the application of deep learning?\n",
    "\n",
    "- [x] The visual cortex is hierarchical\n",
    "- [ ] The visual cortex processes information in a sequential manner \n",
    "- [ ] The visual cortex is highly complex\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "What function does an artificial neuron compute?\n",
    "\n",
    "- [ ] The sum of all inputs \n",
    "- [x] The weighted sum of all inputs and their corresponding weight values\n",
    "- [ ] The weighted product of all inputs values and their corresponding weight values\n",
    "\n",
    "**Question 4**\n",
    "\n",
    "Which of these logical functions can be computed by a perceptron?\n",
    "\n",
    "- [x] AND \n",
    "- [x] OR\n",
    "- [x] NOR \n",
    "- [ ] XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61851c8",
   "metadata": {},
   "source": [
    "### History of NN 2\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "Where do we apply non-linear activation functions?\n",
    "\n",
    "- [ ] After the calculation of the weighted sum of all weights and inputs and before application of the bias \n",
    "- [ ] After we calculate the product of each input and weight \n",
    "- [x] After the calculation of the weighted sum of all weights and inputs and application of the bias\n",
    "- [ ] Directly on the weights of each neuron\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "What is the task of the learning rate in gradient descent?\n",
    "\n",
    "- [X] It controls the step size of the update \n",
    "- [ ] It can be used to measure the difficulty of learning a task \n",
    "- [ ] It controls the rate of direction changes during weight updates\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "Backpropagation based on gradient descent algorithms is\n",
    "\n",
    "- [ ] A convex optimization problem\n",
    "- [x] A non-convex optimization problem\n",
    "\n",
    "**Question 4**\n",
    "\n",
    "Why can we use the backpropagation algorithm to calculate the gradients of arbitrary deep neural networks?\n",
    "\n",
    "- [ ] Because we can directly calculate the derivative of any layer with respect to the output \n",
    "- [x] Because the chain-rule of calculus\n",
    "- [ ] Because we compute only linear functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f91363e",
   "metadata": {},
   "source": [
    "### History of NN 3\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "Which statements about the exploding gradient phenomenon in deep neural networks are true?\n",
    "\n",
    "- [ ] the gradient explodes if the value of the weights is smaller than 1 \n",
    "- [x] the gradient in a deep neural network explode if the values of the gradient are greater than 1\n",
    "- [x] exploding gradients lead to very quick divergence of the model under training\n",
    "- [x] exploding gradients can be mitigated by clipping the gradients\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Which of these gates are part of a LSTM unit?\n",
    "\n",
    "- [ ] State Gate\n",
    "- [x] Input Gate \n",
    "- [x] Output Gate\n",
    "- [x] Forget Gate\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "Why does the usage of a sigmoid unit lead to a vanishing gradient?\n",
    "\n",
    "- [x] The derivate of the function is max 0.25\n",
    "- [ ] The function is monotonically decreasing\n",
    "- [ ] The derivative of the function is max 2\n",
    "- [ ] The function is monotonically rising\n",
    "\n",
    "**Question 4**\n",
    "\n",
    "What is the derivate of the ReLU function for inputs x < 0 ? \n",
    "- [x] 0\n",
    "- [ ] a \n",
    "- [ ] 0.01\n",
    "- [ ] f(x) + a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d9b485",
   "metadata": {},
   "source": [
    "### History of NN 4\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "What happens if we continue to stack layers on a \"plain\" convolutional neural networks, such as AlexNet?\n",
    "\n",
    "- [x] The training error and test error increase\n",
    "- [ ] The training error and test error decease\n",
    "- [ ] The training error increases, while the test error decreases\n",
    "- [ ] The training error decreases, while the test error increases\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "How does the ResNet architecture overcome the problems of very deep neural networks?\n",
    "\n",
    "- [ ] By using only 3 x 3 convolutions throughout the entire network\n",
    "- [ ] By using mixture of tanh and ReLU activation functions\n",
    "- [x] By adding a shortcut connection that leads to better gradient flow\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "Which of these ResNet variants is the largest officially existing one?\n",
    "\n",
    "- [x] ResNet-1202\n",
    "- [ ] ResNet-275\n",
    "- [ ] ResNet-152\n",
    "- [ ] ResNet-50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc78e1d",
   "metadata": {},
   "source": [
    "### SGD\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "How does the weight update using SGD work?\n",
    "\n",
    "1. We forward a batch of samples through the network and calculate the loss  \n",
    "2. We calculate the derivative $\\partial f/\\partial \\theta$ of each computed function w.r.t. inputs and weights \n",
    "3. Each weight is updated using the formula $\\theta_{n+1} = \\theta_n - \\eta \\partial f / \\partial \\theta$\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "What does the hyperparameter $\\eta$ control?\n",
    "\n",
    "- [ ] The starting position of the weight updates in the optimization landscape of the computed function\n",
    "- [x] The learning rate, meaning the step size of weight updates\n",
    "- [ ] The maximum number of layers that are trained by SGD\n",
    "- [ ] The gradient magnitude, meaning the maximum allowed gradient of each function \n",
    "\n",
    "**Question 3**\n",
    "\n",
    "What happens if $\\eta$ is set to a too small value?\n",
    "\n",
    "- [x] The optimization might not find the global minimum because it can not jump over small local maxima\n",
    "- [x] Convergence of the optimization can be very slow \n",
    "- [ ] The optimization fails to converge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003670e5",
   "metadata": {},
   "source": [
    "### Computational Graph\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "An add gate is a gradient...\n",
    "\n",
    "- [x] distributor\n",
    "- [ ] switcher\n",
    "- [ ] router\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Consider the following computational graph, what is the derivative of e w.r.t. a?\n",
    "\n",
    "a = 1, b = 2, f = 10  \n",
    "$c = ab$, $d = b+f$, $e = cd$.\n",
    "\n",
    "- [ ] 2\n",
    "- [ ] 14\n",
    "- [x] 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d81e73",
   "metadata": {},
   "source": [
    "### CNN\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "Which of the following statements about convolutions is true?\n",
    "\n",
    "- [ ] A significant prior of convolutional neural networks is globality\n",
    "- [x] Each neuron is connected just to a small region of the input\n",
    "- [x] Weight sharing in convolutional layers reduces the amount of parameters and calculations\n",
    "- [ ] each convolutional filter is only used for specific region of the input\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Given a 7x7 input, what is the shape if we apply a 3x3 convolution with stride and padding 1?\n",
    "\n",
    "$$\n",
    "O_w = \\frac{I_w - f_w + 2pad_w}{stride} + 1 = \\frac{7 - 3 + 2}{1} + 1 = 7\n",
    "$$\n",
    "\n",
    "- [x] 7x7\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "Given a 9x9 input, can we use a convolution with 3x3 filter and stride 4?\n",
    "\n",
    "$$\n",
    "O_w = \\frac{I_w - f_w + 2pad_w}{stride} + 1 = \\frac{9 - 3 + 0}{4} + 1 = 2.5\n",
    "$$\n",
    "\n",
    "- [x] No \n",
    "- [ ] Yes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
