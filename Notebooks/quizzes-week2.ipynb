{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ad1f9a",
   "metadata": {},
   "source": [
    "# Week 2 - CNN Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc44d4e9",
   "metadata": {},
   "source": [
    "## 1. Notations\n",
    "\n",
    "### 1.1 Pioneering Work\n",
    "\n",
    "- Definition of convolution and pooling layers: Work of Fukushima in 1982, Neocognitron.\n",
    "- Backpropagation applied to handwritten recognition: Work of LeCun in 1989\n",
    "- Publication of the algorithm LeNet5: Conerstone of modern CNN structure. Year was 1998.\n",
    "\n",
    "### 1.2 Architectures\n",
    "\n",
    "1. LeNet-5\n",
    "\n",
    "- Use Tanh as activation function \n",
    "\n",
    "2. AlexNet\n",
    "\n",
    "- The first to stack conv. layers directly on top of one another.\n",
    "- Two regularization techniques: Dropout (50% rate) and data augmentation\n",
    "- Use of a competitive normalization step after the ReLU of layers C1 and C3\n",
    "- 62.5 million parameters (6% from conv. layers and 94% from FC layers)\n",
    "\n",
    "3. ZFNet\n",
    "\n",
    "- Similar to AlexNet with a narrower first conv. layer, and more maps in the following conv. layers.\n",
    "- Reduced the error in 5% compared to AlexNet\n",
    "\n",
    "\n",
    "4. VGG-19 Net (Visual Geometry Group - Oxford University)\n",
    "\n",
    "- Introduces a modular design concept: stages and blocks\n",
    "- According to the author, the use of many consecutive conv. layers introduces a non-linearity while \n",
    "  the number of parameters is reduced.\n",
    "\n",
    "5. GoogLeNet / Inception Net\n",
    "\n",
    "- Great performance due to deeper network using inception modules\n",
    "- 10 times fewer parameters than AlexNet, 6 millions (according to author, 5 millions)\n",
    "- Inception modules: capture patterns along depth, bottleneck layers, acts like a two-layer neural network across the     image\n",
    "\n",
    "6. InceptionNet V2\n",
    "\n",
    "- Uses BatchNorm for each conv layer \n",
    "- Increases non-linearities\n",
    "\n",
    "7. ResNet\n",
    "\n",
    "- Use of residual connections -> Better gradient flow\n",
    "- Possibility of training extreme deeper networks\n",
    "- Demonstrated that network degradation can occur\n",
    "- For deeper networks (> 50 layers), it uses the bottleneck block to suppress both computational complex and the number    of parameters\n",
    "\n",
    "8. ResNeXt\n",
    "\n",
    "9. SENet\n",
    "\n",
    "- Adds a small neural network - SE block (3 layers, global averaging pooling layer, hidden dense layer with relu, and      dense output layer with sigmoid)\n",
    "- SE blocks focus on depth dimension -> recalibrate feature maps\n",
    "\n",
    "10. Attention Networks\n",
    "\n",
    "- Groundbreaking papper from Google team: \"Attention is all you need\"\n",
    "\n",
    "11. NASNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea9121f",
   "metadata": {},
   "source": [
    "## 2. Quizzes\n",
    "\n",
    "### 2.1 CNN Architectures 1\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "Which of these components were a part of the famous LeNet-5 architecture by LeCun et al.?\n",
    "\n",
    "- [x] 5 x 5 Convolution\n",
    "- [ ] Batch Normalization \n",
    "- [x] Fully Connected Layer \n",
    "- [ ] 3 x 3 Convolution\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Which of the following components is most likely to have the biggest influence on the success of the AlexNet?\n",
    "\n",
    "- [x] The usage of ReLU as activation function\n",
    "- [ ] The usage of the max-pooling operation\n",
    "- [ ] The usage of fully connected layers with large number of neurons\n",
    "- [ ] The usage of 3 x 3 convolutions in conjunction with a 5 x 5 and 11 x 11 convolution layers\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "How does Dropout work?\n",
    "\n",
    "- [x] It randomly sets some neurons to zero during the forward pass\n",
    "- [ ] It randomly sets some neurons to zero during the backward pass \n",
    "- [ ] It randomly sets some neurons to one during the forward pass \n",
    "- [ ] It randomly sets some neurons to one during the backward pass\n",
    "\n",
    "### 2.2 CNN Architectures 2\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "What is an insight that one could draw from investigating the ZFNet architecture?\n",
    "\n",
    "- [ ] A large number of neurons in the fully connected layer is important to achieve good results\n",
    "- [ ] It is not necessary to use networks with more than 8 layers\n",
    "- [x] if you reduce the spatial size of a feature map by ,e.g., using pooling, you should increasing the dimensionality of the feature map\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "What was one of the most important insights we can draw from the VGG architecture?\n",
    "\n",
    "- [ ] Bootleneck blocks helps to squeeze feature information and concentrate only on the most important aspects\n",
    "- [ ] Convolutional neural networks do not need to be deep to learn hierarchical representations of visual data\n",
    "- [x] Using only 3x3 convolutional layers allows the usage of more non-linearities. Thus, it is possible to learn more meaningful features.\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "Which of these networks architectures has the lowest number of parameters?\n",
    "\n",
    "- [x] GoogLeNet with 22 layers\n",
    "- [ ] AlexNet with 8 layers\n",
    "- [ ] VGG-19 with 19 layers\n",
    "\n",
    "### 2.3 CNN Architectures 3\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "What is the core concept of an inception module?\n",
    "\n",
    "- [ ] To regularize the learning process and enable a deeper neural network to converge \n",
    "- [x] To decouple the learning process of channel correlation and spatial correlation \n",
    "- [ ] To decouple the correlation of input data and learned weights\n",
    "- [ ] To improve the feature representation obtained by a single convolutional layer\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Which of these statements describe benefits of using Batch Normalization?\n",
    "\n",
    "- [x] Improve accuracy\n",
    "- [ ] Improve memory usage, by using less memory \n",
    "- [x] Improve convergence speed \n",
    "- [ ] Improve speed of a forward pass\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "Which of the following components can be found in Batch Normalization?\n",
    "\n",
    "- [x] The calculation of the mini-batch variance \n",
    "- [x] The application of learnable scale and shift parameters \n",
    "- [x] The calculation of the mini-batch mean \n",
    "- [x] The normalization of the mini-batch using the calculated mean and variance\n",
    "\n",
    "**Question 4**\n",
    "\n",
    "Batch Normalization is effective due to the reduction of internal covariate shift. \n",
    "\n",
    "- [ ] Correct\n",
    "- [x] Incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b274a39",
   "metadata": {},
   "source": [
    "### 2.4 CNN Architectures 4\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "What are possible weaknesses of the InceptionNet V2 architecture?\n",
    "\n",
    "- [x] Over-engineering of the network structure\n",
    "- [ ] marginal accuracy improvements over the original InceptionNet architecture (less than 2%)\n",
    "- [ ] Abandonment of decoupled learning of channel correlation and spatial correlation\n",
    "- [x] Inconsistent kernel shapes\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "What is the purpose of a bottleneck block in the ResNet architecture?\n",
    "\n",
    "- [ ] the bottleneck is used to remove unused features from the feature maps\n",
    "- [ ] the bottleneck is used to encourage the network to focus only on the most important feature information\n",
    "- [x] the bottleneck suppresses both computational complexity and number of parameters\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "Which of the following factors may lead to the larger popularity of ResNet compared to InceptionNet?\n",
    "\n",
    "- [x] ResNet's pre-trained models are supported by almost all deep learning frameworks\n",
    "- [ ] ResNet has been introduced by Google AI \n",
    "- [x] ResNet has a concise design\n",
    "- [x] ResNet has been open source since its introduction\n",
    "\n",
    "### 2.5 CNN Architectures 5\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "What is the core idea of the ResNeXt architecture?\n",
    "\n",
    "- [ ] The introduction of inception modules into the residual architecture of ResNet\n",
    "- [ ] Replacement of the initial 7x7 convolution in the stem with a set of 3x3 convolutions\n",
    "- [x] The replacement of ResNet's three-layer convolutional block with a parallel stack of blocks\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Given a feature map shape 15x15x32 (WxHxC), we want to apply a GroupConvolution with a group number of 2, \n",
    "a kernel of 3x3, stride 1, padding 1. What is the size of the output feature map (WxHxC)?\n",
    "\n",
    "- [ ] 15x15x32\n",
    "- [ ] 15x15x8\n",
    "- [ ] 15x15x16\n",
    "- [x] 15x15x2\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "What is the goal of the SENet architecture?\n",
    "\n",
    "- [ ] To improve representational power of the network by introducing shortcut connections\n",
    "- [ ] To improve accuracy by adding blocks specifically engineered for the task of image classification\n",
    "- [ ] To improve the convergence speed and accuracy of the model by using thinner feature maps and shuffle operations\n",
    "- [x] To improve the representational power of the model by introducing a channel-wise attention mechanism\n",
    "\n",
    "\n",
    "### 2.6 CNN Architectures 6\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "What does NAS stand for?\n",
    "\n",
    "- [ ] Neural Architecture Service\n",
    "- [x] Neural Architecture Search \n",
    "- [ ] Network Automation Service\n",
    "- [ ] Network Search Strategy\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Which machine learning method is used when training a network using the NAS strategy?\n",
    "\n",
    "- [ ] Variational Autoencoders\n",
    "- [ ] Generative Adversative Networks\n",
    "- [x] Reinforcement Learning\n",
    "- [ ] Residual Learning\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "What is the core principle behind neural architecture search?\n",
    "\n",
    "- [ ] Fully random sampling of neural architectures while hopping that they performe well on the task at hand\n",
    "- [ ] Manual architecture design aided by proposals from the computer for the task at hand\n",
    "- [x] Guided sampling, training, and validation of neural architectures that are probable to obtain good performance on the task at hand\n",
    "- [ ] Outsourcing of architecture design, training and validation to crowd workers\n",
    "\n",
    "### 2.7 Transformer \n",
    "\n",
    "**Question 1**\n",
    "\n",
    "What does attention capture / describe?\n",
    "\n",
    "- [x] Correlation between tokens\n",
    "- [ ] parts of images that can safely be ignored\n",
    "- [ ] text tokens that do not convey much semantic information\n",
    "- [x] \"interesting\" receptive fields on the images\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Which form of advanced attention is used in a transformer model?\n",
    "\n",
    "- [x] self-attention\n",
    "- [ ] backwards-attention\n",
    "- [ ] neighbor-attention \n",
    "- [ ] visual-attention\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "Which function is used to calculate the attention weights?\n",
    "\n",
    "- [ ] sigmoid\n",
    "- [ ] tanh\n",
    "- [x] softmax \n",
    "- [ ] cross entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59a330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
